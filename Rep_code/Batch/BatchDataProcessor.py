"""
BatchDataProcessor.py
æ™ºèƒ½é‹å«æ•°æ®æ‰¹å¤„ç†å™¨ - å®Œæ•´ä¿®æ­£ç‰ˆ
å·²ä¿®å¤æ–‡ä»¶ååŒ¹é…é—®é¢˜ï¼šjogging_s1_merged_model_ready.csv è€Œä¸æ˜¯ jogging_s1_model_ready.csv
"""

import os
import re
import yaml
import time
import pandas as pd
import numpy as np
import json
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
import traceback
from dataclasses import dataclass
from enum import Enum
import logging
from logging.handlers import RotatingFileHandler
import warnings
warnings.filterwarnings('ignore')

# ==================== å¯¼å…¥ä½ çš„ç°æœ‰å‡½æ•° ====================
try:
    # å‡è®¾ä½ çš„å‡½æ•°åœ¨åŒä¸€ä¸ªç›®å½•æˆ–å¯ä»¥å¯¼å…¥
    from Dataextract import extract_right_foot_data
    from CheckDatainformation import DataChecker
    from normalize_data import DataNormalizer
    print("âœ… æˆåŠŸå¯¼å…¥æ‰€æœ‰å¤„ç†å‡½æ•°")
except ImportError as e:
    print(f"âš ï¸  å¯¼å…¥å‡½æ•°æ—¶è­¦å‘Š: {e}")
    print("è¯·ç¡®ä¿ä½ çš„ä¸‰ä¸ªå‡½æ•°æ–‡ä»¶åœ¨å¯å¯¼å…¥è·¯å¾„ä¸­")
    
    # å®šä¹‰å ä½å‡½æ•°ä»¥é¿å…é”™è¯¯
    def extract_right_foot_data(csv_path, save_extracted=True, output_folder="subjectRepro1"):
        """å ä½å‡½æ•°"""
        print(f"å ä½: æå–æ•°æ® {csv_path}")
        return None
    
    class DataChecker:
        """å ä½ç±»"""
        def __init__(self, csv_path, output_dir=None):
            self.csv_path = csv_path
            
        def run(self):
            print(f"å ä½: æ£€æŸ¥æ•°æ® {self.csv_path}")
            return "placeholder_params.json"
    
    class DataNormalizer:
        """å ä½ç±»"""
        def __init__(self, params_path=None):
            self.params = {}
            
        def normalize_all(self, df, apply_filter=True, window_size=5):
            print("å ä½: å½’ä¸€åŒ–æ•°æ®")
            return df
            
        def save_normalized_data(self, df, output_path, save_features_only=False):
            print(f"å ä½: ä¿å­˜åˆ° {output_path}")
            return output_path

# ==================== é…ç½®ç±» ====================

class ProcessingMode(Enum):
    """å¤„ç†æ¨¡å¼"""
    AUTO = "auto"           # è‡ªåŠ¨æ£€æµ‹ï¼Œå­˜åœ¨åˆ™è·³è¿‡
    FORCE = "force_all"     # å¼ºåˆ¶é‡æ–°å¤„ç†æ‰€æœ‰
    MISSING = "missing_only" # åªå¤„ç†ç¼ºå¤±çš„æ–‡ä»¶


@dataclass
class BatchConfig:
    """æ‰¹å¤„ç†é…ç½®"""
    # åŸºç¡€è·¯å¾„
    base_path: str = r"D:\TG0\PublicData_Rep\Smart_Insole_Database"
    
    # è¦å¤„ç†çš„å—è¯•è€…
    subjects: List[int] = None
    
    # è¦å¤„ç†çš„åŠ¨ä½œï¼ˆæŒ‰æ–‡ä»¶åå‰ç¼€ï¼‰
    activities: List[str] = None
    
    # å¤„ç†æ¨¡å¼
    processing_mode: ProcessingMode = ProcessingMode.AUTO
    
    # æ˜¯å¦è·³è¿‡å·²å­˜åœ¨çš„æ–‡ä»¶
    skip_existing: bool = True
    
    # å‡ºé”™æ—¶æ˜¯å¦ç»§ç»­
    continue_on_error: bool = True
    
    # æœ€å¤§é‡è¯•æ¬¡æ•°
    max_retries: int = 3
    
    # æ—¥å¿—çº§åˆ«
    log_level: str = "INFO"
    
    # æ˜¯å¦ä¿å­˜è¯¦ç»†æ—¥å¿—
    save_detailed_log: bool = True
    
    # æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡
    show_progress: bool = True
    
    # å„é˜¶æ®µé…ç½®
    stage1_config: Dict = None
    stage2_config: Dict = None
    stage3_config: Dict = None
    
    def __post_init__(self):
        if self.subjects is None:
            self.subjects = [1, 2, 3, 4, 5]
        if self.activities is None:
            self.activities = [
                "jogging",
                "jump_fb", 
                "jump_inplace",
                "squatting",
                "swaying",
                "walking"
            ]
        if self.stage1_config is None:
            self.stage1_config = {
                "output_folder_pattern": "subjectRepro{subject_id}",
                "save_extracted": True
            }
        if self.stage2_config is None:
            self.stage2_config = {
                "params_dir_pattern": "{output_folder}/Param/{activity}_s{subject_id}_merged_preprocess_params",
                "params_filename_pattern": "{activity}_s{subject_id}_merged_model_ready_params.json"
            }
        if self.stage3_config is None:
            self.stage3_config = {
                "norm_dir_pattern": "{output_folder}/norm",
                "norm_filename_pattern": "{activity}_s{subject_id}_merged_normalized.csv",
                "apply_filter": True,
                "window_size": 5
            }


# ==================== æ–‡ä»¶è·¯å¾„ç®¡ç†å™¨ ====================

class PathManager:
    """ç®¡ç†æ‰€æœ‰æ–‡ä»¶è·¯å¾„çš„åˆ›å»ºå’Œè§£æ"""
    
    @staticmethod
    def parse_filename(filename: str) -> Dict[str, str]:
        """
        è§£æåŸå§‹æ–‡ä»¶åï¼Œæå–åŠ¨ä½œå’Œå—è¯•è€…ID
        
        å‚æ•°:
            filename: æ–‡ä»¶åï¼Œå¦‚ "jogging_s1_merged.csv"
            
        è¿”å›:
            åŒ…å«åŠ¨ä½œå’Œå—è¯•è€…IDçš„å­—å…¸
        """
        # åŒ¹é…æ¨¡å¼: {åŠ¨ä½œ}_s{æ•°å­—}_merged.csv
        pattern = r"^(?P<activity>[a-zA-Z_]+)_s(?P<subject_id>\d+)_merged\.csv$"
        match = re.match(pattern, filename)
        
        if not match:
            # å°è¯•å…¶ä»–å¯èƒ½çš„æ¨¡å¼
            patterns = [
                r"^(?P<activity>[a-zA-Z_]+)_(?P<subject_id>\d+)\.csv$",
                r"^(?P<activity>[a-zA-Z_]+)_subject(?P<subject_id>\d+)\.csv$",
            ]
            
            for pat in patterns:
                match = re.match(pat, filename)
                if match:
                    break
        
        if not match:
            raise ValueError(f"æ— æ³•è§£ææ–‡ä»¶å: {filename}")
        
        return {
            "activity": match.group("activity"),
            "subject_id": match.group("subject_id")
        }
    
    @staticmethod
    def build_all_paths(config: BatchConfig, raw_file_path: str) -> Dict[str, str]:
        """
        æ„å»ºæ‰€æœ‰ç›¸å…³æ–‡ä»¶è·¯å¾„ - ä¿®æ­£ç‰ˆæœ¬ï¼ˆåŒ¹é…å®é™…æ–‡ä»¶åï¼‰
        
        å‚æ•°:
            config: é…ç½®å¯¹è±¡
            raw_file_path: åŸå§‹æ–‡ä»¶è·¯å¾„
            
        è¿”å›:
            åŒ…å«æ‰€æœ‰è·¯å¾„çš„å­—å…¸
        """
        # æå–æ–‡ä»¶åå’Œç›®å½•ä¿¡æ¯
        raw_file = Path(raw_file_path)
        filename = raw_file.name  # ä¾‹å¦‚: jogging_s1_merged.csv
        
        # è§£æåŠ¨ä½œå’Œå—è¯•è€…ID
        info = PathManager.parse_filename(filename)
        activity = info["activity"]
        subject_id = info["subject_id"]
        
        # åŸºç¡€åç§°ï¼ˆå»æ‰.csvï¼‰
        base_name = filename.replace('.csv', '')  # jogging_s1_merged
        
        # æ„å»ºåŸºç¡€è·¯å¾„
        base_path = Path(config.base_path)
        
        # ===== é˜¶æ®µ1è·¯å¾„ =====
        # subjectRepro1/
        stage1_folder = base_path / f"subjectRepro{subject_id}"
        
        # æ³¨æ„ï¼šDataextract.pyä¿å­˜çš„æ–‡ä»¶åæ˜¯å¸¦æœ‰"_extracted_right_foot"å’Œ"_model_ready"çš„
        stage1_file1 = stage1_folder / f"{base_name}_extracted_right_foot.csv"  # æå–æ•°æ®
        stage1_file2 = stage1_folder / f"{base_name}_model_ready.csv"          # æ¨¡å‹è®­ç»ƒæ•°æ®
        
        # ===== é˜¶æ®µ2è·¯å¾„ =====
        # subjectRepro1/Param/jogging_s1_merged_preprocess_params/
        params_dir = stage1_folder / "Param" / f"{base_name}_preprocess_params"
        
        # å‚æ•°æ–‡ä»¶: subjectRepro1/Param/jogging_s1_merged_preprocess_params/jogging_s1_merged_model_ready_params.json
        params_file = params_dir / f"{base_name}_model_ready_params.json"
        
        # ===== é˜¶æ®µ3è·¯å¾„ =====
        # subjectRepro1/norm/
        norm_dir = stage1_folder / "norm"
        
        # å½’ä¸€åŒ–æ–‡ä»¶: subjectRepro1/norm/jogging_s1_merged_normalized.csv
        norm_file = norm_dir / f"{base_name}_normalized.csv"
        
        return {
            "raw_file": str(raw_file_path),
            "subject_id": subject_id,
            "activity": activity,
            "base_name": base_name,  # jogging_s1_merged
            
            # é˜¶æ®µ1
            "stage1_folder": str(stage1_folder),
            "stage1_file1": str(stage1_file1),  # æå–æ•°æ® (å¸¦_extracted_right_foot)
            "stage1_file2": str(stage1_file2),  # æ¨¡å‹è®­ç»ƒæ•°æ® (å¸¦_model_ready)
            
            # é˜¶æ®µ2
            "stage2_params_dir": str(params_dir),
            "stage2_params_file": str(params_file),
            
            # é˜¶æ®µ3
            "stage3_norm_dir": str(norm_dir),
            "stage3_norm_file": str(norm_file),
            
            # çŠ¶æ€æ–‡ä»¶
            "status_file": str(stage1_folder / f"status_{base_name}.json")
        }


# ==================== æ–‡ä»¶æ‰«æå™¨ ====================

class FileScanner:
    """æ‰«æåŸå§‹æ–‡ä»¶"""
    
    @staticmethod
    def scan_raw_files(config: BatchConfig) -> List[Dict[str, Any]]:
        """
        æ‰«ææ‰€æœ‰åŸå§‹æ–‡ä»¶
        
        è¿”å›:
            æ–‡ä»¶ä¿¡æ¯åˆ—è¡¨
        """
        base_path = Path(config.base_path)
        files_info = []
        
        for subject_id in config.subjects:
            subject_folder = base_path / f"subject_{subject_id}"
            
            if not subject_folder.exists():
                print(f"è­¦å‘Š: æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {subject_folder}")
                continue
            
            # æ‰«æè¯¥æ–‡ä»¶å¤¹ä¸‹çš„æ‰€æœ‰CSVæ–‡ä»¶
            for csv_file in subject_folder.glob("*.csv"):
                filename = csv_file.name
                
                try:
                    # è§£ææ–‡ä»¶å
                    info = PathManager.parse_filename(filename)
                    activity = info["activity"]
                    
                    # æ£€æŸ¥æ˜¯å¦åœ¨è¦å¤„ç†çš„åŠ¨ä½œåˆ—è¡¨ä¸­
                    if activity in config.activities:
                        # æ„å»ºæ‰€æœ‰è·¯å¾„
                        paths = PathManager.build_all_paths(config, str(csv_file))
                        
                        files_info.append({
                            "raw_file": str(csv_file),
                            "subject_id": subject_id,
                            "activity": activity,
                            "paths": paths,
                            "status": "pending",  # pending, processing, success, failed, skipped
                            "stage1_status": "pending",
                            "stage2_status": "pending", 
                            "stage3_status": "pending",
                            "error_messages": [],
                            "start_time": None,
                            "end_time": None,
                            "processing_time": None
                        })
                        
                except ValueError as e:
                    print(f"è­¦å‘Š: è·³è¿‡æ–‡ä»¶ {filename}: {e}")
                except Exception as e:
                    print(f"è­¦å‘Š: å¤„ç†æ–‡ä»¶ {filename} æ—¶å‡ºé”™: {e}")
        
        return files_info


# ==================== å¤„ç†æµæ°´çº¿ ====================

class ProcessingPipeline:
    """å¤„ç†æµæ°´çº¿ï¼ŒæŒ‰é¡ºåºæ‰§è¡Œä¸‰ä¸ªé˜¶æ®µ"""
    
    def __init__(self, config: BatchConfig):
        self.config = config
        self.logger = self._setup_logger()
    
    def _setup_logger(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—"""
        logger = logging.getLogger("BatchProcessor")
        logger.setLevel(getattr(logging, self.config.log_level))
        
        # æ§åˆ¶å°å¤„ç†å™¨
        console_handler = logging.StreamHandler()
        console_formatter = logging.Formatter(
            '%(asctime)s - %(levelname)s - %(message)s',
            datefmt='%H:%M:%S'
        )
        console_handler.setFormatter(console_formatter)
        logger.addHandler(console_handler)
        
        # æ–‡ä»¶å¤„ç†å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.config.save_detailed_log:
            log_file = f"batch_processing_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
            file_handler = RotatingFileHandler(log_file, maxBytes=10*1024*1024, backupCount=5)
            file_formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            file_handler.setFormatter(file_formatter)
            logger.addHandler(file_handler)
        
        return logger
    
    def check_skip_stage(self, file_info: Dict, stage: str) -> Tuple[bool, str]:
        """
        æ£€æŸ¥æ˜¯å¦åº”è¯¥è·³è¿‡æŸä¸ªé˜¶æ®µ - ä¿®æ­£ç‰ˆæœ¬
        
        è¿”å›:
            (æ˜¯å¦è·³è¿‡, åŸå› )
        """
        paths = file_info["paths"]
        
        if self.config.processing_mode == ProcessingMode.FORCE:
            return False, "å¼ºåˆ¶é‡æ–°å¤„ç†æ¨¡å¼"
        
        # æ£€æŸ¥é˜¶æ®µç‰¹å®šçš„æ–‡ä»¶æ˜¯å¦å­˜åœ¨ - ä½¿ç”¨æ­£ç¡®çš„è·¯å¾„
        if stage == "stage1":
            # paths["stage1_file2"] å·²ç»æ˜¯æ­£ç¡®çš„è·¯å¾„: jogging_s1_merged_model_ready.csv
            if Path(paths["stage1_file2"]).exists() and self.config.skip_existing:
                return True, f"æ–‡ä»¶å·²å­˜åœ¨: {paths['stage1_file2']}"
                
        elif stage == "stage2":
            if Path(paths["stage2_params_file"]).exists() and self.config.skip_existing:
                return True, f"æ–‡ä»¶å·²å­˜åœ¨: {paths['stage2_params_file']}"
                
        elif stage == "stage3":
            if Path(paths["stage3_norm_file"]).exists() and self.config.skip_existing:
                return True, f"æ–‡ä»¶å·²å­˜åœ¨: {paths['stage3_norm_file']}"
        
        return False, "éœ€è¦å¤„ç†"
    
    def run_stage1(self, file_info: Dict) -> Tuple[bool, str]:
        """è¿è¡Œé˜¶æ®µ1ï¼šæå–å³è„šæ•°æ®"""
        paths = file_info["paths"]
        raw_file = paths["raw_file"]
        output_folder = paths["stage1_folder"]
        
        try:
            self.logger.info(f"é˜¶æ®µ1å¼€å§‹: {raw_file}")
            
            # åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹
            os.makedirs(output_folder, exist_ok=True)
            
            # è°ƒç”¨ä½ çš„æå–å‡½æ•°
            result = extract_right_foot_data(
                csv_path=raw_file,
                save_extracted=True,
                output_folder=output_folder
            )
            
            if result is None:
                return False, "æå–å‡½æ•°è¿”å›None"
            
            # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶æ˜¯å¦å­˜åœ¨ - ä½¿ç”¨æ­£ç¡®çš„æ–‡ä»¶å
            if not Path(paths["stage1_file2"]).exists():
                # å†æ£€æŸ¥ä¸€ä¸‹å…¶ä»–å¯èƒ½çš„æ–‡ä»¶å
                alt_file = paths["stage1_file2"].replace("_merged_model_ready.csv", "_model_ready.csv")
                if Path(alt_file).exists():
                    self.logger.warning(f"æ‰¾åˆ°æ–‡ä»¶ä½†æ–‡ä»¶åä¸åŒ: {alt_file}")
                    # é‡å‘½åä¸ºæ ‡å‡†æ–‡ä»¶å
                    Path(alt_file).rename(paths["stage1_file2"])
                else:
                    return False, f"è¾“å‡ºæ–‡ä»¶ä¸å­˜åœ¨: {paths['stage1_file2']}"
            
            self.logger.info(f"é˜¶æ®µ1å®Œæˆ: {paths['stage1_file2']}")
            return True, "æˆåŠŸ"
            
        except Exception as e:
            error_msg = f"é˜¶æ®µ1å¤±è´¥: {str(e)}"
            self.logger.error(error_msg)
            traceback.print_exc()
            return False, error_msg
    
    def run_stage2(self, file_info: Dict) -> Tuple[bool, str]:
        """è¿è¡Œé˜¶æ®µ2ï¼šæ£€æŸ¥æ•°æ®å¹¶ç”Ÿæˆå‚æ•°"""
        paths = file_info["paths"]
        model_ready_file = paths["stage1_file2"]
        params_dir = paths["stage2_params_dir"]
        
        try:
            self.logger.info(f"é˜¶æ®µ2å¼€å§‹: {model_ready_file}")
            
            # åˆ›å»ºå‚æ•°æ–‡ä»¶å¤¹
            os.makedirs(params_dir, exist_ok=True)
            
            # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not Path(model_ready_file).exists():
                return False, f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {model_ready_file}"
            
            # è°ƒç”¨ä½ çš„æ£€æŸ¥å‡½æ•°
            checker = DataChecker(
                csv_path=model_ready_file,
                output_dir=params_dir
            )
            
            # è¿è¡Œæ£€æŸ¥
            param_file = checker.run()
            
            # æ£€æŸ¥å‚æ•°æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not Path(param_file).exists():
                return False, f"å‚æ•°æ–‡ä»¶ä¸å­˜åœ¨: {param_file}"
            
            self.logger.info(f"é˜¶æ®µ2å®Œæˆ: {param_file}")
            return True, "æˆåŠŸ"
            
        except Exception as e:
            error_msg = f"é˜¶æ®µ2å¤±è´¥: {str(e)}"
            self.logger.error(error_msg)
            traceback.print_exc()
            return False, error_msg
    
    def run_stage3(self, file_info: Dict) -> Tuple[bool, str]:
        """è¿è¡Œé˜¶æ®µ3ï¼šå½’ä¸€åŒ–å¤„ç†"""
        paths = file_info["paths"]
        model_ready_file = paths["stage1_file2"]
        params_file = paths["stage2_params_file"]
        norm_dir = paths["stage3_norm_dir"]
        norm_file = paths["stage3_norm_file"]
        
        try:
            self.logger.info(f"é˜¶æ®µ3å¼€å§‹: {model_ready_file}")
            
            # åˆ›å»ºå½’ä¸€åŒ–æ–‡ä»¶å¤¹
            os.makedirs(norm_dir, exist_ok=True)
            
            # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not Path(model_ready_file).exists():
                return False, f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {model_ready_file}"
            
            if not Path(params_file).exists():
                return False, f"å‚æ•°æ–‡ä»¶ä¸å­˜åœ¨: {params_file}"
            
            # è°ƒç”¨ä½ çš„å½’ä¸€åŒ–å‡½æ•°
            normalizer = DataNormalizer(params_path=params_file)
            
            # åŠ è½½æ•°æ®
            df_raw = pd.read_csv(model_ready_file)
            
            # åº”ç”¨å½’ä¸€åŒ–
            df_norm = normalizer.normalize_all(
                df_raw,
                apply_filter=self.config.stage3_config["apply_filter"],
                window_size=self.config.stage3_config["window_size"]
            )
            
            # ä¿å­˜å½’ä¸€åŒ–æ•°æ®
            normalizer.save_normalized_data(
                df_norm,
                norm_file,
                save_features_only=True
            )
            
            # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not Path(norm_file).exists():
                return False, f"è¾“å‡ºæ–‡ä»¶ä¸å­˜åœ¨: {norm_file}"
            
            self.logger.info(f"é˜¶æ®µ3å®Œæˆ: {norm_file}")
            return True, "æˆåŠŸ"
            
        except Exception as e:
            error_msg = f"é˜¶æ®µ3å¤±è´¥: {str(e)}"
            self.logger.error(error_msg)
            traceback.print_exc()
            return False, error_msg
    
    def process_file(self, file_info: Dict, retry_count: int = 0) -> Dict:
        """
        å¤„ç†å•ä¸ªæ–‡ä»¶ï¼ˆä¸‰ä¸ªé˜¶æ®µï¼‰
        
        å‚æ•°:
            file_info: æ–‡ä»¶ä¿¡æ¯
            retry_count: å½“å‰é‡è¯•æ¬¡æ•°
            
        è¿”å›:
            æ›´æ–°åçš„æ–‡ä»¶ä¿¡æ¯
        """
        file_info["start_time"] = datetime.now()
        paths = file_info["paths"]
        
        try:
            # ===== é˜¶æ®µ1 =====
            skip_stage1, reason1 = self.check_skip_stage(file_info, "stage1")
            if skip_stage1:
                file_info["stage1_status"] = "skipped"
                file_info["stage1_reason"] = reason1
                self.logger.info(f"è·³è¿‡é˜¶æ®µ1: {reason1}")
            else:
                success1, message1 = self.run_stage1(file_info)
                file_info["stage1_status"] = "success" if success1 else "failed"
                file_info["stage1_message"] = message1
                
                if not success1:
                    file_info["status"] = "failed"
                    file_info["error_messages"].append(f"é˜¶æ®µ1: {message1}")
                    return file_info
            
            # ===== é˜¶æ®µ2 =====
            skip_stage2, reason2 = self.check_skip_stage(file_info, "stage2")
            if skip_stage2:
                file_info["stage2_status"] = "skipped"
                file_info["stage2_reason"] = reason2
                self.logger.info(f"è·³è¿‡é˜¶æ®µ2: {reason2}")
            else:
                success2, message2 = self.run_stage2(file_info)
                file_info["stage2_status"] = "success" if success2 else "failed"
                file_info["stage2_message"] = message2
                
                if not success2:
                    file_info["status"] = "failed"
                    file_info["error_messages"].append(f"é˜¶æ®µ2: {message2}")
                    return file_info
            
            # ===== é˜¶æ®µ3 =====
            skip_stage3, reason3 = self.check_skip_stage(file_info, "stage3")
            if skip_stage3:
                file_info["stage3_status"] = "skipped"
                file_info["stage3_reason"] = reason3
                self.logger.info(f"è·³è¿‡é˜¶æ®µ3: {reason3}")
            else:
                success3, message3 = self.run_stage3(file_info)
                file_info["stage3_status"] = "success" if success3 else "failed"
                file_info["stage3_message"] = message3
                
                if not success3:
                    file_info["status"] = "failed"
                    file_info["error_messages"].append(f"é˜¶æ®µ3: {message3}")
                    return file_info
            
            # æ‰€æœ‰é˜¶æ®µæˆåŠŸ
            file_info["status"] = "success"
            
        except Exception as e:
            file_info["status"] = "failed"
            error_msg = f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {str(e)}"
            file_info["error_messages"].append(error_msg)
            self.logger.error(error_msg)
            traceback.print_exc()
            
            # é‡è¯•é€»è¾‘
            if retry_count < self.config.max_retries:
                self.logger.warning(f"å‡†å¤‡é‡è¯• (ç¬¬{retry_count+1}æ¬¡): {paths['raw_file']}")
                time.sleep(2 ** retry_count)  # æŒ‡æ•°é€€é¿
                return self.process_file(file_info, retry_count + 1)
        
        finally:
            file_info["end_time"] = datetime.now()
            if file_info["start_time"] and file_info["end_time"]:
                file_info["processing_time"] = (file_info["end_time"] - file_info["start_time"]).total_seconds()
            
            # ä¿å­˜çŠ¶æ€æ–‡ä»¶
            self.save_file_status(file_info)
        
        return file_info
    
    def save_file_status(self, file_info: Dict):
        """ä¿å­˜æ–‡ä»¶å¤„ç†çŠ¶æ€"""
        try:
            status_file = file_info["paths"]["status_file"]
            status_dir = os.path.dirname(status_file)
            os.makedirs(status_dir, exist_ok=True)
            
            # å‡†å¤‡çŠ¶æ€ä¿¡æ¯
            status_info = {
                "subject_id": file_info["subject_id"],
                "activity": file_info["activity"],
                "raw_file": file_info["raw_file"],
                "status": file_info["status"],
                "stage1_status": file_info.get("stage1_status", "pending"),
                "stage2_status": file_info.get("stage2_status", "pending"),
                "stage3_status": file_info.get("stage3_status", "pending"),
                "error_messages": file_info.get("error_messages", []),
                "processing_time": file_info.get("processing_time"),
                "timestamp": datetime.now().isoformat()
            }
            
            with open(status_file, 'w', encoding='utf-8') as f:
                json.dump(status_info, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            self.logger.warning(f"æ— æ³•ä¿å­˜çŠ¶æ€æ–‡ä»¶: {e}")


# ==================== è¿›åº¦è·Ÿè¸ªå™¨ ====================

class ProgressTracker:
    """è·Ÿè¸ªå¤„ç†è¿›åº¦"""
    
    def __init__(self, total_files: int):
        self.total_files = total_files
        self.processed_files = 0
        self.successful = 0
        self.failed = 0
        self.skipped = 0
        self.start_time = datetime.now()
        
        # æŒ‰é˜¶æ®µç»Ÿè®¡
        self.stage_stats = {
            "stage1": {"success": 0, "failed": 0, "skipped": 0},
            "stage2": {"success": 0, "failed": 0, "skipped": 0},
            "stage3": {"success": 0, "failed": 0, "skipped": 0},
        }
        
        # æŒ‰å—è¯•è€…ç»Ÿè®¡
        self.subject_stats = {}
        
        # æŒ‰åŠ¨ä½œç»Ÿè®¡
        self.activity_stats = {}
    
    def update(self, file_info: Dict):
        """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
        self.processed_files += 1
        
        # æ›´æ–°æ€»ä½“çŠ¶æ€
        status = file_info.get("status", "unknown")
        if status == "success":
            self.successful += 1
        elif status == "failed":
            self.failed += 1
        elif status == "skipped":
            self.skipped += 1
        
        # æ›´æ–°é˜¶æ®µç»Ÿè®¡
        for stage in ["stage1", "stage2", "stage3"]:
            stage_status = file_info.get(f"{stage}_status", "unknown")
            if stage_status in ["success", "failed", "skipped"]:
                self.stage_stats[stage][stage_status] += 1
        
        # æ›´æ–°å—è¯•è€…ç»Ÿè®¡
        subject_id = file_info.get("subject_id")
        if subject_id:
            if subject_id not in self.subject_stats:
                self.subject_stats[subject_id] = {"success": 0, "failed": 0, "skipped": 0}
            
            if status in ["success", "failed", "skipped"]:
                self.subject_stats[subject_id][status] += 1
        
        # æ›´æ–°åŠ¨ä½œç»Ÿè®¡
        activity = file_info.get("activity")
        if activity:
            if activity not in self.activity_stats:
                self.activity_stats[activity] = {"success": 0, "failed": 0, "skipped": 0}
            
            if status in ["success", "failed", "skipped"]:
                self.activity_stats[activity][status] += 1
    
    def get_progress(self) -> Dict[str, Any]:
        """è·å–è¿›åº¦ä¿¡æ¯"""
        elapsed = (datetime.now() - self.start_time).total_seconds()
        
        if self.processed_files > 0:
            avg_time_per_file = elapsed / self.processed_files
            remaining_files = self.total_files - self.processed_files
            estimated_remaining = avg_time_per_file * remaining_files if remaining_files > 0 else 0
        else:
            estimated_remaining = 0
        
        # è®¡ç®—è¿›åº¦ç™¾åˆ†æ¯”
        progress_percent = (self.processed_files / self.total_files * 100) if self.total_files > 0 else 0
        
        return {
            "total_files": self.total_files,
            "processed_files": self.processed_files,
            "successful": self.successful,
            "failed": self.failed,
            "skipped": self.skipped,
            "progress_percent": progress_percent,
            "elapsed_time": elapsed,
            "estimated_remaining": estimated_remaining,
            "stage_stats": self.stage_stats,
            "subject_stats": self.subject_stats,
            "activity_stats": self.activity_stats
        }
    
    def display_progress(self, show_detailed: bool = True):
        """æ˜¾ç¤ºè¿›åº¦ä¿¡æ¯"""
        progress = self.get_progress()
        
        # è¿›åº¦æ¡
        bar_length = 50
        filled_length = int(bar_length * progress["progress_percent"] / 100)
        bar = 'â–ˆ' * filled_length + 'â–‘' * (bar_length - filled_length)
        
        print(f"\nğŸ“Š å¤„ç†è¿›åº¦: [{bar}] {progress['progress_percent']:.1f}%")
        print(f"ğŸ“ æ–‡ä»¶: {progress['processed_files']}/{progress['total_files']}")
        print(f"âœ… æˆåŠŸ: {progress['successful']} | â­ï¸ è·³è¿‡: {progress['skipped']} | âŒ å¤±è´¥: {progress['failed']}")
        
        # æ—¶é—´ä¿¡æ¯
        elapsed_str = self._format_time(progress["elapsed_time"])
        remaining_str = self._format_time(progress["estimated_remaining"])
        print(f"â±ï¸  å·²ç”¨: {elapsed_str} | é¢„è®¡å‰©ä½™: {remaining_str}")
        
        if show_detailed and progress["processed_files"] > 0:
            print("\nğŸ“ˆ è¯¦ç»†ç»Ÿè®¡:")
            
            # é˜¶æ®µç»Ÿè®¡
            print("  é˜¶æ®µç»Ÿè®¡:")
            for stage, stats in progress["stage_stats"].items():
                stage_name = stage.replace("stage", "é˜¶æ®µ")
                print(f"    {stage_name}: âœ…{stats['success']} â­ï¸{stats['skipped']} âŒ{stats['failed']}")
    
    @staticmethod
    def _format_time(seconds: float) -> str:
        """æ ¼å¼åŒ–æ—¶é—´æ˜¾ç¤º"""
        if seconds < 60:
            return f"{seconds:.0f}ç§’"
        elif seconds < 3600:
            minutes = seconds / 60
            return f"{minutes:.0f}åˆ†{seconds%60:.0f}ç§’"
        else:
            hours = seconds / 3600
            minutes = (seconds % 3600) / 60
            return f"{hours:.0f}æ—¶{minutes:.0f}åˆ†"


# ==================== ä¸»æ‰¹å¤„ç†å™¨ ====================

class BatchDataProcessor:
    """ä¸»æ‰¹å¤„ç†å™¨"""
    
    def __init__(self, config: BatchConfig = None):
        self.config = config or BatchConfig()
        self.pipeline = ProcessingPipeline(self.config)
        self.file_scanner = FileScanner()
        self.progress_tracker = None
        self.files_info = []
        self.results = []
    
    def load_config_from_yaml(self, config_file: str):
        """ä»YAMLæ–‡ä»¶åŠ è½½é…ç½®"""
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                config_dict = yaml.safe_load(f)
            
            # æ›´æ–°é…ç½®
            for key, value in config_dict.items():
                if hasattr(self.config, key):
                    if key == "processing_mode":
                        value = ProcessingMode(value)
                    setattr(self.config, key, value)
            
            self.pipeline = ProcessingPipeline(self.config)
            print(f"âœ… ä» {config_file} åŠ è½½é…ç½®")
            
        except Exception as e:
            print(f"âŒ åŠ è½½é…ç½®å¤±è´¥: {e}")
    
    def scan_files(self) -> List[Dict]:
        """æ‰«ææ‰€æœ‰æ–‡ä»¶"""
        print(f"\nğŸ” æ‰«ææ–‡ä»¶...")
        self.files_info = self.file_scanner.scan_raw_files(self.config)
        
        if not self.files_info:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°å¯å¤„ç†çš„æ–‡ä»¶")
            return []
        
        print(f"âœ… æ‰¾åˆ° {len(self.files_info)} ä¸ªåŸå§‹æ–‡ä»¶")
        
        # æŒ‰å—è¯•è€…å’ŒåŠ¨ä½œæ’åº
        self.files_info.sort(key=lambda x: (int(x["subject_id"]), x["activity"]))
        
        # æ˜¾ç¤ºæ–‡ä»¶åˆ—è¡¨
        print("\nğŸ“‹ æ–‡ä»¶åˆ—è¡¨:")
        for i, file_info in enumerate(self.files_info[:10], 1):  # åªæ˜¾ç¤ºå‰10ä¸ª
            print(f"  {i:2d}. {file_info['raw_file']}")
        
        if len(self.files_info) > 10:
            print(f"  ... å’Œ {len(self.files_info) - 10} ä¸ªå…¶ä»–æ–‡ä»¶")
        
        return self.files_info
    
    def process_all(self) -> List[Dict]:
        """å¤„ç†æ‰€æœ‰æ–‡ä»¶"""
        if not self.files_info:
            print("âŒ æ²¡æœ‰å¯å¤„ç†çš„æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œ scan_files()")
            return []
        
        # åˆå§‹åŒ–è¿›åº¦è·Ÿè¸ªå™¨
        self.progress_tracker = ProgressTracker(len(self.files_info))
        
        print(f"\nğŸš€ å¼€å§‹æ‰¹é‡å¤„ç†...")
        print(f"ğŸ“Š æ€»æ–‡ä»¶æ•°: {len(self.files_info)}")
        print(f"âš™ï¸  å¤„ç†æ¨¡å¼: {self.config.processing_mode.value}")
        print(f"â­ï¸  è·³è¿‡å·²å­˜åœ¨: {self.config.skip_existing}")
        print(f"ğŸ” å‡ºé”™ç»§ç»­: {self.config.continue_on_error}")
        print("=" * 60)
        
        self.results = []
        
        for i, file_info in enumerate(self.files_info, 1):
            print(f"\nğŸ“„ å¤„ç†æ–‡ä»¶ {i}/{len(self.files_info)}: {file_info['raw_file']}")
            
            try:
                # å¤„ç†å•ä¸ªæ–‡ä»¶
                result = self.pipeline.process_file(file_info)
                self.results.append(result)
                
                # æ›´æ–°è¿›åº¦
                self.progress_tracker.update(result)
                
                # æ˜¾ç¤ºè¿›åº¦
                if self.config.show_progress and i % 5 == 0:  # æ¯5ä¸ªæ–‡ä»¶æ˜¾ç¤ºä¸€æ¬¡
                    self.progress_tracker.display_progress(show_detailed=False)
                
                # æ˜¾ç¤ºå¤„ç†ç»“æœ
                status = result.get("status", "unknown")
                if status == "success":
                    print(f"  âœ… å®Œæˆ: {result['paths']['stage3_norm_file']}")
                elif status == "failed":
                    print(f"  âŒ å¤±è´¥: {result.get('error_messages', ['æœªçŸ¥é”™è¯¯'])[0]}")
                elif status == "skipped":
                    print(f"  â­ï¸ è·³è¿‡: {result.get('stage1_reason', 'å·²å­˜åœ¨')}")
                
            except Exception as e:
                print(f"  âŒ å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {e}")
                file_info["status"] = "failed"
                file_info["error_messages"].append(f"å¤„ç†å¼‚å¸¸: {str(e)}")
                self.results.append(file_info)
                self.progress_tracker.update(file_info)
                
                if not self.config.continue_on_error:
                    print("âŒ ç”±äºé”™è¯¯è€Œåœæ­¢å¤„ç†")
                    break
        
        # æ˜¾ç¤ºæœ€ç»ˆè¿›åº¦
        if self.config.show_progress:
            self.progress_tracker.display_progress(show_detailed=True)
        
        return self.results
    
    def generate_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆå¤„ç†æŠ¥å‘Š"""
        if not self.results:
            return {"error": "æ²¡æœ‰å¤„ç†ç»“æœ"}
        
        # æ”¶é›†ç»Ÿè®¡ä¿¡æ¯
        total_files = len(self.results)
        successful = sum(1 for r in self.results if r.get("status") == "success")
        failed = sum(1 for r in self.results if r.get("status") == "failed")
        skipped = sum(1 for r in self.results if r.get("status") == "skipped")
        
        # æŒ‰é˜¶æ®µç»Ÿè®¡
        stage_stats = {"stage1": {}, "stage2": {}, "stage3": {}}
        for stage in stage_stats.keys():
            stage_stats[stage]["success"] = sum(1 for r in self.results if r.get(f"{stage}_status") == "success")
            stage_stats[stage]["failed"] = sum(1 for r in self.results if r.get(f"{stage}_status") == "failed")
            stage_stats[stage]["skipped"] = sum(1 for r in self.results if r.get(f"{stage}_status") == "skipped")
        
        # æŒ‰å—è¯•è€…ç»Ÿè®¡
        subject_stats = {}
        for result in self.results:
            subject_id = result.get("subject_id")
            if subject_id not in subject_stats:
                subject_stats[subject_id] = {"total": 0, "success": 0, "failed": 0, "skipped": 0}
            
            subject_stats[subject_id]["total"] += 1
            status = result.get("status", "unknown")
            if status in ["success", "failed", "skipped"]:
                subject_stats[subject_id][status] += 1
        
        # æŒ‰åŠ¨ä½œç»Ÿè®¡
        activity_stats = {}
        for result in self.results:
            activity = result.get("activity")
            if activity not in activity_stats:
                activity_stats[activity] = {"total": 0, "success": 0, "failed": 0, "skipped": 0}
            
            activity_stats[activity]["total"] += 1
            status = result.get("status", "unknown")
            if status in ["success", "failed", "skipped"]:
                activity_stats[activity][status] += 1
        
        # é”™è¯¯æ±‡æ€»
        errors = []
        for result in self.results:
            if result.get("status") == "failed":
                errors.append({
                    "file": result.get("raw_file"),
                    "errors": result.get("error_messages", []),
                    "subject_id": result.get("subject_id"),
                    "activity": result.get("activity")
                })
        
        # æ—¶é—´ç»Ÿè®¡
        processing_times = [r.get("processing_time", 0) for r in self.results if r.get("processing_time")]
        avg_time = sum(processing_times) / len(processing_times) if processing_times else 0
        total_time = sum(processing_times)
        
        report = {
            "summary": {
                "total_files": total_files,
                "successful": successful,
                "failed": failed,
                "skipped": skipped,
                "success_rate": successful / total_files * 100 if total_files > 0 else 0,
                "total_processing_time": total_time,
                "average_time_per_file": avg_time
            },
            "stage_stats": stage_stats,
            "subject_stats": subject_stats,
            "activity_stats": activity_stats,
            "errors": errors,
            "timestamp": datetime.now().isoformat(),
            "config": {
                "base_path": self.config.base_path,
                "subjects": self.config.subjects,
                "activities": self.config.activities,
                "processing_mode": self.config.processing_mode.value,
                "skip_existing": self.config.skip_existing
            }
        }
        
        return report
    
    def save_report(self, report_file: str = None):
        """ä¿å­˜å¤„ç†æŠ¥å‘Š"""
        if not report_file:
            report_file = f"batch_processing_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        report = self.generate_report()
        
        try:
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… å¤„ç†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
            return report_file
        except Exception as e:
            print(f"âŒ ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")
            return None
    
    def print_summary(self):
        """æ‰“å°å¤„ç†æ‘˜è¦"""
        if not self.results:
            print("æ²¡æœ‰å¤„ç†ç»“æœ")
            return
        
        report = self.generate_report()
        summary = report["summary"]
        
        print("\n" + "=" * 60)
        print("ğŸ“Š æ‰¹å¤„ç†å®Œæˆæ‘˜è¦")
        print("=" * 60)
        print(f"ğŸ“ æ€»æ–‡ä»¶æ•°: {summary['total_files']}")
        print(f"âœ… æˆåŠŸå¤„ç†: {summary['successful']} ({summary['success_rate']:.1f}%)")
        print(f"â­ï¸  è·³è¿‡: {summary['skipped']}")
        print(f"âŒ å¤±è´¥: {summary['failed']}")
        print(f"â±ï¸  æ€»å¤„ç†æ—¶é—´: {summary['total_processing_time']:.1f}ç§’")
        print(f"ğŸ“ˆ å¹³å‡æ¯æ–‡ä»¶: {summary['average_time_per_file']:.1f}ç§’")
        
        # æ˜¾ç¤ºå¤±è´¥çš„è¯¦ç»†ä¿¡æ¯
        if report["errors"]:
            print(f"\nâŒ å¤±è´¥æ–‡ä»¶ ({len(report['errors'])}ä¸ª):")
            for error in report["errors"][:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                print(f"  â€¢ {error['file']}")
                for err_msg in error["errors"][:2]:  # åªæ˜¾ç¤ºå‰2ä¸ªé”™è¯¯
                    print(f"    - {err_msg}")
            
            if len(report["errors"]) > 5:
                print(f"    ... å’Œ {len(report['errors']) - 5} ä¸ªå…¶ä»–å¤±è´¥")


# ==================== ä¸»å‡½æ•°å’Œå‘½ä»¤è¡Œæ¥å£ ====================

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸ—ï¸  æ™ºèƒ½é‹å«æ•°æ®æ‰¹å¤„ç†å™¨ - ä¿®æ­£ç‰ˆ")
    print("å·²ä¿®å¤æ–‡ä»¶ååŒ¹é…é—®é¢˜")
    print("=" * 60)
    
    # åˆ›å»ºé…ç½®
    config = BatchConfig(
        base_path=r"D:\TG0\PublicData_Rep\Smart_Insole_Database",
        subjects=[1, 2, 3, 4, 5],
        processing_mode=ProcessingMode.AUTO,
        skip_existing=True,
        continue_on_error=True,
        log_level="INFO",
        show_progress=True
    )
    
    # åˆ›å»ºæ‰¹å¤„ç†å™¨
    processor = BatchDataProcessor(config)
    
    # æ‰«ææ–‡ä»¶
    files = processor.scan_files()
    if not files:
        return
    
    # å¤„ç†æ‰€æœ‰æ–‡ä»¶
    results = processor.process_all()
    
    # ç”ŸæˆæŠ¥å‘Š
    processor.print_summary()
    
    # ä¿å­˜æŠ¥å‘Š
    report_file = processor.save_report()
    
    print("\n" + "=" * 60)
    print("ğŸ‰ æ‰¹å¤„ç†å®Œæˆï¼")
    print("=" * 60)
    
    # æ˜¾ç¤ºä¸‹ä¸€æ­¥å»ºè®®
    print("\nğŸ“‹ ä¸‹ä¸€æ­¥å»ºè®®:")
    print("1. æ£€æŸ¥å¤±è´¥çš„æ–‡ä»¶ï¼ŒæŸ¥çœ‹é”™è¯¯æ—¥å¿—")
    print("2. æŸ¥çœ‹å¤„ç†æŠ¥å‘Š:", report_file)
    print("3. å½’ä¸€åŒ–æ•°æ®ä¿å­˜åœ¨å„ subjectReproX/norm/ æ–‡ä»¶å¤¹ä¸‹")
    print("4. å¯ä»¥ä¿®æ”¹é…ç½®é‡æ–°è¿è¡Œå¤±è´¥çš„æ–‡ä»¶")


def create_sample_config():
    """åˆ›å»ºç¤ºä¾‹é…ç½®æ–‡ä»¶"""
    config = {
        "base_path": r"D:\TG0\PublicData_Rep\Smart_Insole_Database",
        "subjects": [1, 2, 3, 4, 5],
        "activities": [
            "jogging",
            "jump_fb",
            "jump_inplace",
            "squatting",
            "swaying",
            "walking"
        ],
        "processing_mode": "auto",  # auto, force_all, missing_only
        "skip_existing": True,
        "continue_on_error": True,
        "max_retries": 3,
        "log_level": "INFO",
        "save_detailed_log": True,
        "show_progress": True,
        "stage1_config": {
            "output_folder_pattern": "subjectRepro{subject_id}",
            "save_extracted": True
        },
        "stage2_config": {
            "params_dir_pattern": "{output_folder}/Param/{activity}_s{subject_id}_merged_preprocess_params",
            "params_filename_pattern": "{activity}_s{subject_id}_merged_model_ready_params.json"
        },
        "stage3_config": {
            "norm_dir_pattern": "{output_folder}/norm",
            "norm_filename_pattern": "{activity}_s{subject_id}_merged_normalized.csv",
            "apply_filter": True,
            "window_size": 5
        }
    }
    
    with open("batch_config.yaml", 'w', encoding='utf-8') as f:
        yaml.dump(config, f, default_flow_style=False, allow_unicode=True, indent=2)
    
    print("âœ… ç¤ºä¾‹é…ç½®æ–‡ä»¶å·²åˆ›å»º: batch_config.yaml")


# ==================== å‘½ä»¤è¡Œæ¥å£ ====================

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="æ™ºèƒ½é‹å«æ•°æ®æ‰¹å¤„ç†å™¨")
    parser.add_argument("--base_path", type=str, help="åŸºç¡€æ•°æ®è·¯å¾„")
    parser.add_argument("--subjects", type=str, help="è¦å¤„ç†çš„å—è¯•è€…ï¼Œå¦‚ '1,2,3,4,5'")
    parser.add_argument("--mode", type=str, choices=["auto", "force_all", "missing_only"], default="auto", help="å¤„ç†æ¨¡å¼")
    parser.add_argument("--skip_existing", type=lambda x: x.lower() == "true", default=True, help="æ˜¯å¦è·³è¿‡å·²å­˜åœ¨çš„æ–‡ä»¶")
    parser.add_argument("--config", type=str, help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--create_config", action="store_true", help="åˆ›å»ºç¤ºä¾‹é…ç½®æ–‡ä»¶")
    parser.add_argument("--scan_only", action="store_true", help="åªæ‰«ææ–‡ä»¶ï¼Œä¸å¤„ç†")
    
    args = parser.parse_args()
    
    if args.create_config:
        create_sample_config()
        sys.exit(0)
    
    # åˆ›å»ºé…ç½®
    config = BatchConfig()
    
    # åº”ç”¨å‘½ä»¤è¡Œå‚æ•°
    if args.base_path:
        config.base_path = args.base_path
    
    if args.subjects:
        config.subjects = [int(s.strip()) for s in args.subjects.split(",")]
    
    config.processing_mode = ProcessingMode(args.mode)
    config.skip_existing = args.skip_existing
    
    # åˆ›å»ºå¤„ç†å™¨
    processor = BatchDataProcessor(config)
    
    # åŠ è½½é…ç½®æ–‡ä»¶ï¼ˆå¦‚æœæŒ‡å®šï¼‰
    if args.config and os.path.exists(args.config):
        processor.load_config_from_yaml(args.config)
    
    # æ‰«ææ–‡ä»¶
    files = processor.scan_files()
    
    if args.scan_only or not files:
        sys.exit(0)
    
    # å¤„ç†æ–‡ä»¶
    results = processor.process_all()
    
    # ç”ŸæˆæŠ¥å‘Š
    processor.print_summary()
    processor.save_report()