"""
LearnMethod.py
å­¦ä¹ æœºåˆ¶æ¨¡å—ï¼šæä¾›çµæ´»é…ç½®çš„è®­ç»ƒç»„ä»¶ - ä¿®å¤ç‰ˆ
"""
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from typing import Dict, List, Tuple, Optional, Any
import warnings
import time

# ==================== 1. é…ç½®ç®¡ç†ç±» ====================
class TrainingConfig:
    """è®­ç»ƒé…ç½®ç®¡ç†ç±» - æä¾›çµæ´»çš„é…ç½®æ¥å£"""
    
    def __init__(self, **kwargs):
        """åˆå§‹åŒ–é…ç½®ï¼Œæ”¯æŒå¤šç§å‚æ•°è®¾ç½®æ–¹å¼"""
        
        # ===== å…ˆè®¾ç½®é»˜è®¤å€¼ =====
        self._set_defaults()
        
        # ===== å†æ›´æ–°ç”¨æˆ·æä¾›çš„é…ç½® =====
        self._update_config(kwargs)
        
        # ===== æœ€åå¤„ç†ä¾èµ–å…³ç³»ï¼ˆå¦‚è®¾å¤‡æ£€æµ‹ï¼‰ =====
        self._post_process()
    
    def _set_defaults(self):
        """è®¾ç½®æ‰€æœ‰é»˜è®¤å€¼"""
        # åŸºç¡€è®­ç»ƒå‚æ•°
        self.batch_size = 64
        self.learning_rate = 0.001
        self.weight_decay = 1e-8
        self.epochs = 100
        
        # å­¦ä¹ ç‡è°ƒåº¦
        self.use_lr_scheduler = True
        self.lr_scheduler_type = 'plateau'  # 'plateau', 'step', 'cosine'
        self.lr_patience = 3
        self.lr_factor = 0.1
        self.lr_min = 1e-6
        
        # æ—©åœæœºåˆ¶
        self.use_early_stopping = True
        self.early_stop_patience = 10
        self.early_stop_delta = 1e-4
        
        # æ¢¯åº¦å¤„ç†
        self.use_gradient_clip = True
        self.grad_clip_norm = 1.0
        
        # æŸå¤±å‡½æ•°
        self.loss_function = 'mse'  # 'mse', 'mae', 'huber'
        self.huber_delta = 1.0
        
        # ä¼˜åŒ–å™¨
        self.optimizer = 'adam'  # 'adam', 'sgd', 'adamw'
        self.sgd_momentum = 0.9
        
        # è®¾å¤‡
        self.device = 'auto'  # 'auto', 'cuda', 'cpu'
        
        # æ•°æ®åˆ’åˆ†
        self.train_ratio = 0.7
        self.val_ratio = 0.15
        self.test_ratio = 0.15
        
        # æ—¥å¿—å’Œä¿å­˜
        self.save_checkpoints = True
        self.checkpoint_freq = 10
        self.verbose = True
    
    def _update_config(self, user_config: Dict):
        """æ›´æ–°é…ç½®å‚æ•°"""
        for key, value in user_config.items():
            if hasattr(self, key):
                setattr(self, key, value)
            else:
                warnings.warn(f"è­¦å‘Š: æœªçŸ¥é…ç½®å‚æ•° '{key}'ï¼Œå°†è¢«å¿½ç•¥")
    
    def _post_process(self):
        """é…ç½®åå¤„ç†"""
        # è‡ªåŠ¨æ£€æµ‹è®¾å¤‡
        if self.device == 'auto':
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        elif isinstance(self.device, str):
            self.device = torch.device(self.device)
        # å¦‚æœå·²ç»æ˜¯torch.deviceå¯¹è±¡ï¼Œä¿æŒä¸å˜
    
    def update(self, **kwargs):
        """åŠ¨æ€æ›´æ–°é…ç½®"""
        self._update_config(kwargs)
        self._post_process()  # é‡æ–°åå¤„ç†
        return self
    
    def to_dict(self) -> Dict:
        """å°†é…ç½®è½¬æ¢ä¸ºå­—å…¸"""
        config_dict = {}
        for attr_name in dir(self):
            if not attr_name.startswith('_') and not callable(getattr(self, attr_name)):
                attr_value = getattr(self, attr_name)
                # å¦‚æœæ˜¯torch.deviceå¯¹è±¡ï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²
                if isinstance(attr_value, torch.device):
                    config_dict[attr_name] = str(attr_value)
                else:
                    config_dict[attr_name] = attr_value
        return config_dict
    
    def __str__(self) -> str:
        """å‹å¥½çš„é…ç½®æ˜¾ç¤º"""
        config_str = "=== è®­ç»ƒé…ç½® ===\n"
        
        # å®šä¹‰é…ç½®åˆ†ç±»
        categories = {
            'åŸºç¡€è®­ç»ƒ': ['batch_size', 'learning_rate', 'weight_decay', 'epochs'],
            'å­¦ä¹ ç‡è°ƒåº¦': ['use_lr_scheduler', 'lr_scheduler_type', 'lr_patience', 'lr_factor', 'lr_min'],
            'æ—©åœæœºåˆ¶': ['use_early_stopping', 'early_stop_patience', 'early_stop_delta'],
            'æ¢¯åº¦å¤„ç†': ['use_gradient_clip', 'grad_clip_norm'],
            'æŸå¤±å‡½æ•°': ['loss_function', 'huber_delta'],
            'ä¼˜åŒ–å™¨': ['optimizer', 'sgd_momentum'],
            'è®¾å¤‡': ['device'],
            'æ•°æ®åˆ’åˆ†': ['train_ratio', 'val_ratio', 'test_ratio'],
            'æ—¥å¿—ä¿å­˜': ['save_checkpoints', 'checkpoint_freq', 'verbose']
        }
        
        for category, params in categories.items():
            config_str += f"\nã€{category}ã€‘\n"
            for param in params:
                if hasattr(self, param):
                    value = getattr(self, param)
                    if isinstance(value, torch.device):
                        value = str(value)
                    config_str += f"  {param}: {value}\n"
        
        return config_str

# ==================== 2. å·¥å‚æ¨¡å¼åˆ›å»ºè®­ç»ƒç»„ä»¶ ====================
class TrainingComponentFactory:
    """è®­ç»ƒç»„ä»¶å·¥å‚ - æ ¹æ®é…ç½®åˆ›å»ºå„ç§ç»„ä»¶"""
    
    @staticmethod
    def create_loss_function(config: TrainingConfig) -> nn.Module:
        """åˆ›å»ºæŸå¤±å‡½æ•°"""
        loss_type = config.loss_function.lower()
        
        if loss_type == 'mse':
            return nn.MSELoss()
        elif loss_type == 'mae':
            return nn.L1Loss()
        elif loss_type == 'huber':
            return nn.HuberLoss(delta=config.huber_delta)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æŸå¤±å‡½æ•°: {loss_type}")
    
    @staticmethod
    def create_optimizer(model: nn.Module, config: TrainingConfig) -> optim.Optimizer:
        """åˆ›å»ºä¼˜åŒ–å™¨"""
        optimizer_type = config.optimizer.lower()
        
        if optimizer_type == 'adam':
            return optim.Adam(
                model.parameters(),
                lr=config.learning_rate,
                weight_decay=config.weight_decay
            )
        elif optimizer_type == 'adamw':
            return optim.AdamW(
                model.parameters(),
                lr=config.learning_rate,
                weight_decay=config.weight_decay
            )
        elif optimizer_type == 'sgd':
            return optim.SGD(
                model.parameters(),
                lr=config.learning_rate,
                momentum=config.sgd_momentum,
                weight_decay=config.weight_decay
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ä¼˜åŒ–å™¨: {optimizer_type}")
    
    @staticmethod
    def create_lr_scheduler(optimizer: optim.Optimizer, config: TrainingConfig) -> Optional[optim.lr_scheduler._LRScheduler]:
        """åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨"""
        if not config.use_lr_scheduler:
            return None
        
        scheduler_type = config.lr_scheduler_type.lower()
        
        if scheduler_type == 'plateau':
            return optim.lr_scheduler.ReduceLROnPlateau(
                optimizer,
                mode='min',
                factor=config.lr_factor,
                patience=config.lr_patience,
                min_lr=config.lr_min,
                verbose=config.verbose
            )
        elif scheduler_type == 'step':
            return optim.lr_scheduler.StepLR(
                optimizer,
                step_size=20,
                gamma=0.1
            )
        elif scheduler_type == 'cosine':
            return optim.lr_scheduler.CosineAnnealingLR(
                optimizer,
                T_max=config.epochs
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„è°ƒåº¦å™¨ç±»å‹: {scheduler_type}")

# ==================== 3. æ™ºèƒ½è®­ç»ƒç®¡ç†å™¨ ====================
class SmartTrainingManager:
    """æ™ºèƒ½è®­ç»ƒç®¡ç†å™¨ - è‡ªåŠ¨ç®¡ç†è®­ç»ƒæµç¨‹"""
    
    def __init__(self, model: nn.Module, config: TrainingConfig = None):
        """åˆå§‹åŒ–è®­ç»ƒç®¡ç†å™¨"""
        self.model = model
        self.config = config if config else TrainingConfig()
        
        # åˆ›å»ºæ‰€æœ‰è®­ç»ƒç»„ä»¶
        self._create_components()
        
        # è®­ç»ƒçŠ¶æ€
        self.current_epoch = 0
        self.best_val_loss = float('inf')
        self.best_model_state = None
        self.early_stop_counter = 0
        
        # è®­ç»ƒå†å²è®°å½•
        self.history = {
            'train_loss': [],
            'val_loss': [],
            'learning_rate': [],
            'epoch_time': [],
            'train_time': [],
            'val_time': []
        }
        
        if self.config.verbose:
            print("âœ… æ™ºèƒ½è®­ç»ƒç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
            print(self.config)
    
    def _create_components(self):
        """åˆ›å»ºæ‰€æœ‰è®­ç»ƒç»„ä»¶"""
        factory = TrainingComponentFactory()
        
        # æŸå¤±å‡½æ•°
        self.criterion = factory.create_loss_function(self.config)
        
        # ä¼˜åŒ–å™¨
        self.optimizer = factory.create_optimizer(self.model, self.config)
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.lr_scheduler = factory.create_lr_scheduler(self.optimizer, self.config)
        
        # ç§»åŠ¨æ¨¡å‹åˆ°è®¾å¤‡
        self.model.to(self.config.device)
        
        if self.config.verbose:
            print("\nğŸ“¦ è®­ç»ƒç»„ä»¶è¯¦æƒ…:")
            print(f"   æŸå¤±å‡½æ•°: {self.config.loss_function.upper()}")
            print(f"   ä¼˜åŒ–å™¨: {self.config.optimizer.upper()}")
            if self.lr_scheduler:
                print(f"   å­¦ä¹ ç‡è°ƒåº¦å™¨: {self.config.lr_scheduler_type.upper()}")
            else:
                print(f"   å­¦ä¹ ç‡è°ƒåº¦å™¨: æ— ")
            print(f"   è®¾å¤‡: {self.config.device}")
    
    def train_epoch(self, train_loader) -> float:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0
        batch_count = 0
        epoch_start_time = time.time()
        
        for batch_idx, (capsense, imu, labels) in enumerate(train_loader):
            batch_start_time = time.time()
            
            # ç§»åŠ¨åˆ°è®¾å¤‡
            capsense = capsense.to(self.config.device)
            imu = imu.to(self.config.device)
            labels = labels.to(self.config.device)
            
            # å‰å‘ä¼ æ’­
            self.optimizer.zero_grad()
            predictions = self.model(capsense, imu)
            loss = self.criterion(predictions, labels)
            
            # åå‘ä¼ æ’­
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            if self.config.use_gradient_clip:
                torch.nn.utils.clip_grad_norm_(
                    self.model.parameters(),
                    self.config.grad_clip_norm
                )
            
            self.optimizer.step()
            
            total_loss += loss.item()
            batch_count += 1
            
            # è¿›åº¦æ˜¾ç¤º
            if self.config.verbose and batch_idx % 10 == 0:
                batch_time = time.time() - batch_start_time
                print(f"   æ‰¹æ¬¡ {batch_idx:4d}/{len(train_loader)} | "
                      f"æŸå¤±: {loss.item():.6f} | "
                      f"æ—¶é—´: {batch_time:.3f}s")
        
        epoch_time = time.time() - epoch_start_time
        avg_loss = total_loss / batch_count if batch_count > 0 else 0
        
        return avg_loss, epoch_time
    
    def validate(self, val_loader) -> float:
        """éªŒè¯æ¨¡å‹"""
        self.model.eval()
        total_loss = 0
        batch_count = 0
        val_start_time = time.time()
        
        with torch.no_grad():
            for capsense, imu, labels in val_loader:
                capsense = capsense.to(self.config.device)
                imu = imu.to(self.config.device)
                labels = labels.to(self.config.device)
                
                predictions = self.model(capsense, imu)
                loss = self.criterion(predictions, labels)
                
                total_loss += loss.item()
                batch_count += 1
        
        val_time = time.time() - val_start_time
        avg_loss = total_loss / batch_count if batch_count > 0 else 0
        
        return avg_loss, val_time
    
    def fit(self, train_loader, val_loader) -> Dict:
        """å®Œæ•´çš„è®­ç»ƒæµç¨‹"""
        print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
        print(f"è®­ç»ƒè½®æ•°: {self.config.epochs}")
        print(f"æ‰¹æ¬¡å¤§å°: {self.config.batch_size}")
        print(f"è®­ç»ƒæ ·æœ¬: {len(train_loader.dataset)}")
        print(f"éªŒè¯æ ·æœ¬: {len(val_loader.dataset)}")
        print("-" * 60)
        
        total_start_time = time.time()
        
        for epoch in range(self.current_epoch, self.config.epochs):
            self.current_epoch = epoch + 1
            
            if self.config.verbose:
                print(f"\nğŸ“Š Epoch {self.current_epoch}/{self.config.epochs}")
                print("-" * 40)
            
            # è®­ç»ƒ
            train_loss, train_time = self.train_epoch(train_loader)
            
            # éªŒè¯
            val_loss, val_time = self.validate(val_loader)
            
            # æ›´æ–°å­¦ä¹ ç‡
            if self.lr_scheduler:
                if isinstance(self.lr_scheduler, optim.lr_scheduler.ReduceLROnPlateau):
                    self.lr_scheduler.step(val_loss)
                else:
                    self.lr_scheduler.step()
            
            # è·å–å½“å‰å­¦ä¹ ç‡
            current_lr = self.optimizer.param_groups[0]['lr']
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_loss < self.best_val_loss - self.config.early_stop_delta:
                self.best_val_loss = val_loss
                self.best_model_state = self.model.state_dict().copy()
                self.early_stop_counter = 0
                improved = True
            else:
                self.early_stop_counter += 1
                improved = False
            
            # è®°å½•å†å²
            self.history['train_loss'].append(train_loss)
            self.history['val_loss'].append(val_loss)
            self.history['learning_rate'].append(current_lr)
            self.history['epoch_time'].append(train_time + val_time)
            self.history['train_time'].append(train_time)
            self.history['val_time'].append(val_time)
            
            # æ‰“å°ä¿¡æ¯
            if self.config.verbose:
                print(f"  è®­ç»ƒæŸå¤±: {train_loss:.6f} ({train_time:.2f}s)")
                print(f"  éªŒè¯æŸå¤±: {val_loss:.6f} ({val_time:.2f}s)")
                print(f"  å­¦ä¹ ç‡: {current_lr:.6f}")
                print(f"  æœ€ä½³éªŒè¯æŸå¤±: {self.best_val_loss:.6f}")
                if improved:
                    print("  ğŸ¯ æ¨¡å‹æ”¹è¿›!")
                else:
                    print(f"  â³ æ— æ”¹è¿›è®¡æ•°: {self.early_stop_counter}/{self.config.early_stop_patience}")
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            if (self.config.save_checkpoints and 
                self.current_epoch % self.config.checkpoint_freq == 0):
                checkpoint_path = f"checkpoint_epoch_{self.current_epoch}.pth"
                self.save_checkpoint(checkpoint_path)
            
            # æ—©åœæ£€æŸ¥
            if (self.config.use_early_stopping and 
                self.early_stop_counter >= self.config.early_stop_patience):
                if self.config.verbose:
                    print(f"\nâš ï¸ æ—©åœè§¦å‘! è¿ç»­{self.early_stop_counter}è½®æ— æ”¹è¿›")
                break
        
        total_time = time.time() - total_start_time
        
        # åŠ è½½æœ€ä½³æ¨¡å‹
        if self.best_model_state:
            self.model.load_state_dict(self.best_model_state)
        
        if self.config.verbose:
            print("\n" + "=" * 60)
            print("âœ… è®­ç»ƒå®Œæˆ!")
            print(f"   æ€»è®­ç»ƒæ—¶é—´: {total_time:.2f}s ({total_time/60:.2f}åˆ†é’Ÿ)")
            print(f"   æœ€ä½³éªŒè¯æŸå¤±: {self.best_val_loss:.6f}")
            print(f"   æœ€ç»ˆéªŒè¯æŸå¤±: {self.history['val_loss'][-1]:.6f}")
            print(f"   æ€»è®­ç»ƒè½®æ•°: {self.current_epoch}")
            print("=" * 60)
        
        return self.history
    
    def save_checkpoint(self, filepath: str):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': self.current_epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'best_val_loss': self.best_val_loss,
            'history': self.history,
            'config': self.config.to_dict()
        }
        torch.save(checkpoint, filepath)
        
        if self.config.verbose:
            print(f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {filepath}")
    
    def load_checkpoint(self, filepath: str):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        checkpoint = torch.load(filepath, map_location=self.config.device)
        
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.current_epoch = checkpoint['epoch']
        self.best_val_loss = checkpoint['best_val_loss']
        self.history = checkpoint['history']
        
        # æ›´æ–°é…ç½®
        for key, value in checkpoint['config'].items():
            if hasattr(self.config, key):
                setattr(self.config, key, value)
        
        if self.config.verbose:
            print(f"ğŸ“‚ æ£€æŸ¥ç‚¹å·²åŠ è½½: {filepath}")
            print(f"   ä»ç¬¬ {self.current_epoch} è½®ç»§ç»­è®­ç»ƒ")

# ==================== 4. è¯„ä¼°å·¥å…· ====================
class ModelEvaluator:
    """æ¨¡å‹è¯„ä¼°å·¥å…·"""
    
    @staticmethod
    def evaluate(model: nn.Module, test_loader, device) -> Dict:
        """å…¨é¢è¯„ä¼°æ¨¡å‹"""
        model.eval()
        all_predictions = []
        all_labels = []
        
        eval_start_time = time.time()
        
        with torch.no_grad():
            for capsense, imu, labels in test_loader:
                capsense = capsense.to(device)
                imu = imu.to(device)
                labels = labels.to(device)
                
                predictions = model(capsense, imu)
                
                all_predictions.append(predictions.cpu())
                all_labels.append(labels.cpu())
        
        eval_time = time.time() - eval_start_time
        
        # åˆå¹¶ç»“æœ
        predictions = torch.cat(all_predictions, dim=0)
        labels = torch.cat(all_labels, dim=0)
        
        # è®¡ç®—å„ç§æŒ‡æ ‡
        metrics = ModelEvaluator.calculate_all_metrics(predictions, labels)
        
        print(f"âœ… è¯„ä¼°å®Œæˆ ({eval_time:.2f}s)")
        print(f"   æ ·æœ¬æ•°é‡: {len(predictions)}")
        
        return {
            'predictions': predictions,
            'labels': labels,
            'metrics': metrics,
            'eval_time': eval_time
        }
    
    @staticmethod
    def calculate_all_metrics(predictions: torch.Tensor, labels: torch.Tensor) -> Dict:
        """è®¡ç®—æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡"""
        
        def safe_divide(a, b):
            return a / b if b != 0 else 0
        
        # MSE
        mse = nn.MSELoss()(predictions, labels).item()
        
        # MAE
        mae = nn.L1Loss()(predictions, labels).item()
        
        # RMSE
        rmse = np.sqrt(mse)
        
        # NRMSE (è®ºæ–‡ä¸»è¦æŒ‡æ ‡)
        label_range = labels.max() - labels.min()
        nrmse = safe_divide(rmse, label_range) * 100
        
        # RÂ²
        ss_res = torch.sum((labels - predictions) ** 2)
        ss_tot = torch.sum((labels - torch.mean(labels)) ** 2)
        r2 = 1 - safe_divide(ss_res, ss_tot)
        
        # ç›¸å…³ç³»æ•°
        corr_coeffs = []
        for i in range(predictions.shape[1]):
            pred_i = predictions[:, i]
            label_i = labels[:, i]
            corr = np.corrcoef(pred_i.numpy(), label_i.numpy())[0, 1]
            corr_coeffs.append(corr)
        
        # å¹³å‡ç›¸å…³ç³»æ•°
        avg_corr = np.mean(corr_coeffs) if corr_coeffs else 0
        
        return {
            'MSE': mse,
            'MAE': mae,
            'RMSE': rmse,
            'NRMSE (%)': nrmse,
            'RÂ²': r2.item(),
            'Correlation (Fx,Fy,Fz)': corr_coeffs,
            'Avg Correlation': avg_corr
        }
    
    @staticmethod
    def print_metrics(metrics: Dict):
        """æ ¼å¼åŒ–æ‰“å°è¯„ä¼°æŒ‡æ ‡"""
        print("\nğŸ“Š è¯„ä¼°ç»“æœ:")
        print("-" * 40)
        print(f"  MSE:          {metrics['MSE']:.6f}")
        print(f"  MAE:          {metrics['MAE']:.6f}")
        print(f"  RMSE:         {metrics['RMSE']:.6f}")
        print(f"  NRMSE:        {metrics['NRMSE (%)']:.2f}%")
        print(f"  RÂ²:           {metrics['RÂ²']:.4f}")
        print(f"  å¹³å‡ç›¸å…³ç³»æ•°:  {metrics['Avg Correlation']:.4f}")
        print(f"  åˆ†é‡ç›¸å…³ç³»æ•°:  Fx={metrics['Correlation (Fx,Fy,Fz)'][0]:.4f}, "
              f"Fy={metrics['Correlation (Fx,Fy,Fz)'][1]:.4f}, "
              f"Fz={metrics['Correlation (Fx,Fy,Fz)'][2]:.4f}")

# ==================== 5. ä½¿ç”¨ç¤ºä¾‹ ====================
def create_default_config() -> TrainingConfig:
    """åˆ›å»ºé»˜è®¤é…ç½®ï¼ˆè®ºæ–‡å‚æ•°ï¼‰"""
    config = TrainingConfig(
        # è®ºæ–‡ä½¿ç”¨çš„å‚æ•°
        learning_rate=0.0001,  # è®ºæ–‡: 0.0001
        weight_decay=1e-8,     # è®ºæ–‡: 1e-8
        lr_scheduler_type='plateau',  # è®ºæ–‡: ReduceLROnPlateau
        lr_patience=3,         # è®ºæ–‡: 3
        lr_factor=0.1,         # è®ºæ–‡: 0.1
        
        # è®­ç»ƒå‚æ•°
        batch_size=64,
        epochs=100,
        
        # æ—©åœ
        use_early_stopping=True,
        early_stop_patience=10,
        
        # å…¶ä»–
        loss_function='mse',
        optimizer='adam',
        verbose=True
    )
    return config

def create_fast_config() -> TrainingConfig:
    """åˆ›å»ºå¿«é€Ÿæµ‹è¯•é…ç½®"""
    config = TrainingConfig(
        batch_size=32,
        learning_rate=0.001,
        epochs=5,
        use_lr_scheduler=False,
        use_early_stopping=False,
        verbose=True
    )
    return config

# ==================== 6. æµ‹è¯•ä»£ç  ====================
if __name__ == "__main__":
    print("ğŸ§ª æµ‹è¯• LearnMethod.py æ¨¡å—")
    print("=" * 60)
    
    # æµ‹è¯•é…ç½®ç®¡ç†
    print("1. æµ‹è¯•é…ç½®ç®¡ç†...")
    config = TrainingConfig(
        batch_size=128,
        learning_rate=0.01,
        epochs=50
    )
    print(config)
    
    # æµ‹è¯•åŠ¨æ€æ›´æ–°
    config.update(loss_function='huber', huber_delta=0.5)
    print("\næ›´æ–°åçš„é…ç½®:")
    print(config)
    
    # æµ‹è¯•é…ç½®å­—å…¸
    print("\né…ç½®å­—å…¸:")
    print(config.to_dict())
    
    # æµ‹è¯•é¢„è®¾é…ç½®
    print("\n2. æµ‹è¯•é¢„è®¾é…ç½®...")
    paper_config = create_default_config()
    print("è®ºæ–‡é…ç½®:")
    print(paper_config)
    
    fast_config = create_fast_config()
    print("\nå¿«é€Ÿé…ç½®:")
    print(fast_config)
    
    print("\nâœ… LearnMethod.py æ¨¡å—æµ‹è¯•å®Œæˆï¼")