# SmartInsoleDataset_batch.py
import torch
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import numpy as np
from pathlib import Path
import glob
import os
from tqdm import tqdm
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score


class BatchSmartInsoleDataset(Dataset):
    """æ‰¹é‡å¤„ç†çš„æ•°æ®é›†ç±» - æŒ‰è®ºæ–‡è¦æ±‚"""
    
    def __init__(self, file_paths, seq_length=1, cache_dir=None, force_reload=False):
        """
        å‚æ•°:
            file_paths: å½’ä¸€åŒ–CSVæ–‡ä»¶è·¯å¾„åˆ—è¡¨
            seq_length: åºåˆ—é•¿åº¦ï¼ˆè®ºæ–‡ä¸­ä¸º1ï¼‰
            cache_dir: ç¼“å­˜ç›®å½•ï¼Œç”¨äºå­˜å‚¨é¢„å¤„ç†æ•°æ®
            force_reload: æ˜¯å¦å¼ºåˆ¶é‡æ–°åŠ è½½æ•°æ®
        """
        self.file_paths = file_paths
        self.seq_length = seq_length
        self.cache_dir = cache_dir
        
        # ç‰¹å¾åˆ—å®šä¹‰ï¼ˆæŒ‰è®ºæ–‡è¦æ±‚ï¼‰
        self.feature_columns = self._get_feature_columns()
        self.label_columns = ['Fx_norm', 'Fy_norm', 'Fz_norm']
        
        # åŠ è½½æ•°æ®
        self.data, self.labels = self._load_batch_data(file_paths, cache_dir, force_reload)
        
        print(f"âœ… æ•°æ®é›†åŠ è½½å®Œæˆ:")
        print(f"   æ€»æ ·æœ¬æ•°: {len(self.data):,}")
        print(f"   ç‰¹å¾ç»´åº¦: {self.data.shape[1]} (12 Cap + 3 Acc + 2 Gyro = 17)")
        print(f"   æ ‡ç­¾ç»´åº¦: {self.labels.shape[1]} (3 GRFåˆ†é‡)")
    
    def _get_feature_columns(self):
        """æ ¹æ®è®ºæ–‡å®šä¹‰ç‰¹å¾åˆ— - åŒ¹é…å®é™…æ•°æ®è¡¨å¤´"""
        # CapSenseä¼ æ„Ÿå™¨: å®é™…åˆ—åä¸º C0, C1, ..., C11
        cap_cols = [f'C{i}' for i in range(12)]
        
        # åŠ é€Ÿåº¦è®¡: Ax, Ay, Az
        acc_cols = ['Ax', 'Ay', 'Az']
        
        # é™€èºä»ª: Gp, Gr (pitch, roll)
        gyro_cols = ['Gp', 'Gr']
        
        # å…±17ä¸ªç‰¹å¾
        return cap_cols + acc_cols + gyro_cols
    
    def _load_single_file(self, file_path):
        """åŠ è½½å•ä¸ªCSVæ–‡ä»¶"""
        try:
            # ä½¿ç”¨æ›´å¥å£®çš„CSVè¯»å–æ–¹å¼
            try:
                df = pd.read_csv(file_path)
            except pd.errors.ParserError:
                # å°è¯•è·³è¿‡é”™è¯¯è¡Œ
                df = pd.read_csv(file_path, on_bad_lines='skip')
            except Exception as e:
                print(f"âŒ è¯»å–CSVå¤±è´¥ {Path(file_path).name}: {e}")
                return None, None
            
            # æ£€æŸ¥å¿…è¦çš„åˆ—
            missing_features = [col for col in self.feature_columns if col not in df.columns]
            missing_labels = [col for col in self.label_columns if col not in df.columns]
            
            if missing_features:
                print(f"âš ï¸  æ–‡ä»¶ {Path(file_path).name} ç¼ºå°‘ç‰¹å¾åˆ—: {missing_features}")
                return None, None
            
            if missing_labels:
                print(f"âš ï¸  æ–‡ä»¶ {Path(file_path).name} ç¼ºå°‘æ ‡ç­¾åˆ—: {missing_labels}")
                return None, None
            
            # æå–ç‰¹å¾å’Œæ ‡ç­¾
            features = df[self.feature_columns].values.astype(np.float32)
            labels = df[self.label_columns].values.astype(np.float32)
            
            return features, labels
            
        except Exception as e:
            print(f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥ {Path(file_path).name}: {e}")
            return None, None
    
    def _load_batch_data(self, file_paths, cache_dir=None, force_reload=False):
        """æ‰¹é‡åŠ è½½æ‰€æœ‰æ–‡ä»¶æ•°æ®"""
        all_features = []
        all_labels = []
        
        # æ£€æŸ¥ç¼“å­˜
        cache_file = None
        if cache_dir and not force_reload:
            os.makedirs(cache_dir, exist_ok=True)
            file_hash = hash(tuple(sorted(file_paths)))
            cache_file = os.path.join(cache_dir, f"dataset_cache_{file_hash}.npz")
            
            if os.path.exists(cache_file):
                print(f"ğŸ“¦ ä»ç¼“å­˜åŠ è½½æ•°æ®: {cache_file}")
                npz_data = np.load(cache_file)
                all_features = npz_data['features']
                all_labels = npz_data['labels']
                return all_features, all_labels
        
        # æ‰¹é‡åŠ è½½æ•°æ®
        print(f"ğŸ“¥ æ‰¹é‡åŠ è½½ {len(file_paths)} ä¸ªæ–‡ä»¶...")
        
        successful_files = 0
        failed_files = 0
        
        for file_idx, file_path in enumerate(tqdm(file_paths, desc="åŠ è½½æ–‡ä»¶")):
            features, labels = self._load_single_file(file_path)
            
            if features is not None and labels is not None:
                all_features.append(features)
                all_labels.append(labels)
                successful_files += 1
            else:
                failed_files += 1
            
            # å®šæœŸæŠ¥å‘Šè¿›åº¦
            if (file_idx + 1) % 10 == 0:
                print(f"  å·²å¤„ç† {file_idx + 1}/{len(file_paths)} æ–‡ä»¶ï¼ŒæˆåŠŸ: {successful_files}, å¤±è´¥: {failed_files}")
        
        print(f"âœ… æ–‡ä»¶åŠ è½½å®Œæˆ: {successful_files} æˆåŠŸ, {failed_files} å¤±è´¥")
        
        # åˆå¹¶æ‰€æœ‰æ•°æ®
        if all_features:
            all_features = np.vstack(all_features)
            all_labels = np.vstack(all_labels)
        else:
            all_features = np.array([]).reshape(0, len(self.feature_columns))
            all_labels = np.array([]).reshape(0, 3)
        
        # ä¿å­˜ç¼“å­˜
        if cache_file and all_features.size > 0:
            print(f"ğŸ’¾ ä¿å­˜æ•°æ®åˆ°ç¼“å­˜: {cache_file}")
            np.savez_compressed(cache_file, 
                               features=all_features, 
                               labels=all_labels,
                               feature_columns=self.feature_columns,
                               label_columns=self.label_columns)
        
        return all_features, all_labels
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        features = self.data[idx]  # [17]
        
        # æ‹†åˆ†ç‰¹å¾(åŒæµ)
        cap_features = features[:12]      # C0-C11
        imu_features = features[12:]      # Ax,Ay,Az,Gp,Gr
        
        labels = self.labels[idx]         # [3]
        
        return {
            'cap_features': torch.FloatTensor(cap_features),
            'imu_features': torch.FloatTensor(imu_features),
            'labels': torch.FloatTensor(labels)
        }
    
    def get_statistics(self):
        """è·å–æ•°æ®ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'n_samples': len(self.data),
            'feature_shape': self.data.shape[1] if len(self.data) > 0 else 0,
            'label_shape': self.labels.shape[1] if len(self.labels) > 0 else 0,
            'feature_range': (
                float(self.data.min()) if len(self.data) > 0 else 0,
                float(self.data.max()) if len(self.data) > 0 else 1
            ),
            'label_range': (
                float(self.labels.min()) if len(self.labels) > 0 else 0,
                float(self.labels.max()) if len(self.labels) > 0 else 1
            )
        }
        return stats

def create_batch_data_loaders(base_path, split_method='subject', batch_size=32, 
                              cache_dir='./data_cache', force_reload=False):
    """
    æ‰¹é‡åˆ›å»ºæ•°æ®åŠ è½½å™¨
    
    å‚æ•°:
        base_path: æ•°æ®æ ¹ç›®å½•
        split_method: åˆ’åˆ†ç­–ç•¥ ('subject', 'random', 'mixed')
        batch_size: æ‰¹é‡å¤§å°
        cache_dir: ç¼“å­˜ç›®å½•
        force_reload: æ˜¯å¦å¼ºåˆ¶é‡æ–°åŠ è½½
    """
    
    print("="*80)
    print("ğŸ¤– æ‰¹é‡æ•°æ®åŠ è½½å™¨ - æ™ºèƒ½æ•°æ®åˆ’åˆ†ä¸åŠ è½½")
    print("="*80)
    
    # 1. æŸ¥æ‰¾æ‰€æœ‰å½’ä¸€åŒ–æ–‡ä»¶
    pattern = f"{base_path}/subjectRepro*/norm/*_normalized.csv"
    all_files = glob.glob(pattern)
    
    print(f"ğŸ” åœ¨ {base_path} ä¸­æŸ¥æ‰¾æ–‡ä»¶...")
    print(f"æ‰¾åˆ° {len(all_files)} ä¸ªå½’ä¸€åŒ–æ–‡ä»¶")
    
    if len(all_files) == 0:
        raise ValueError(f"æœªæ‰¾åˆ°å½’ä¸€åŒ–æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥è·¯å¾„: {pattern}")
    
    # æ˜¾ç¤ºå‰å‡ ä¸ªæ–‡ä»¶
    print(f"\nğŸ“ æ–‡ä»¶ç¤ºä¾‹:")
    for i, f in enumerate(all_files[:3]):
        print(f"  {i+1}. {Path(f).name}")
    if len(all_files) > 3:
        print(f"  ... è¿˜æœ‰ {len(all_files)-3} ä¸ªæ–‡ä»¶")
    
    # 2. æŒ‰subjectåˆ†ç±»
    print(f"\nğŸ“Š æŒ‰subjectåˆ†ç±»...")
    files_by_subject = {}
    for file_path in all_files:
        # ä»è·¯å¾„æå–subjectç¼–å·
        path_parts = Path(file_path).parts
        for part in path_parts:
            if part.startswith('subjectRepro'):
                subject_id = part.replace('subjectRepro', '')
                if subject_id not in files_by_subject:
                    files_by_subject[subject_id] = []
                files_by_subject[subject_id].append(file_path)
                break
    
    print(f"  æ‰¾åˆ° {len(files_by_subject)} ä¸ªsubjects")
    for subject_id, files in sorted(files_by_subject.items()):
        print(f"    subject_{subject_id}: {len(files)} æ–‡ä»¶")
    
    # 3. åˆ’åˆ†æ•°æ®é›†
    print(f"\nğŸ“ˆ ä½¿ç”¨ '{split_method}' ç­–ç•¥åˆ’åˆ†æ•°æ®é›†")
    
    if split_method == 'subject':
        # ç•™å‡ºä¸€ä¸ªsubjectåšæµ‹è¯•
        subject_ids = list(files_by_subject.keys())
        test_subject = subject_ids[-1]  # ç”¨æœ€åä¸€ä¸ªsubjectæµ‹è¯•
        train_val_subjects = subject_ids[:-1]
        
        # ä»è®­ç»ƒéªŒè¯é›†ä¸­å†åˆ†ä¸€ä¸ªéªŒè¯subject
        val_subject = train_val_subjects[-1]
        train_subjects = train_val_subjects[:-1]
        
        # æ”¶é›†æ–‡ä»¶
        train_files = []
        for subject in train_subjects:
            train_files.extend(files_by_subject[subject])
        
        val_files = files_by_subject[val_subject]
        test_files = files_by_subject[test_subject]
        
        print(f"  è®­ç»ƒsubjects: {train_subjects} ({len(train_files)} æ–‡ä»¶)")
        print(f"  éªŒè¯subject: {val_subject} ({len(val_files)} æ–‡ä»¶)")
        print(f"  æµ‹è¯•subject: {test_subject} ({len(test_files)} æ–‡ä»¶)")
    
    elif split_method == 'random':
        # éšæœºåˆ’åˆ†
        from sklearn.model_selection import train_test_split
        train_files, temp_files = train_test_split(all_files, test_size=0.3, random_state=42)
        val_files, test_files = train_test_split(temp_files, test_size=0.5, random_state=42)
        
        print(f"  éšæœºåˆ’åˆ†: {len(train_files)}è®­ç»ƒ, {len(val_files)}éªŒè¯, {len(test_files)}æµ‹è¯•")
    
    elif split_method == 'mixed':
        # æ··åˆåˆ’åˆ†ï¼šæ¯ä¸ªsubjectéƒ½æœ‰æ•°æ®åœ¨ä¸‰ä¸ªé›†åˆä¸­
        train_files, val_files, test_files = [], [], []
        
        for subject_id, files in files_by_subject.items():
            n_files = len(files)
            n_test = max(1, int(n_files * 0.15))  # 15%æµ‹è¯•
            n_val = max(1, int(n_files * 0.15))   # 15%éªŒè¯
            n_train = n_files - n_test - n_val     # 70%è®­ç»ƒ
            
            # æ‰“ä¹±æ–‡ä»¶
            import random
            random.shuffle(files)
            
            train_files.extend(files[:n_train])
            val_files.extend(files[n_train:n_train+n_val])
            test_files.extend(files[n_train+n_val:])
        
        print(f"  æ··åˆåˆ’åˆ†: {len(train_files)}è®­ç»ƒ, {len(val_files)}éªŒè¯, {len(test_files)}æµ‹è¯•")
    
    else:
        raise ValueError(f"æœªçŸ¥åˆ’åˆ†æ–¹æ³•: {split_method}")
    
    # 4. åˆ›å»ºæ•°æ®é›†
    print(f"\nğŸ”„ åˆ›å»ºæ•°æ®é›†...")
    
    train_dataset = BatchSmartInsoleDataset(
        train_files, 
        seq_length=1,
        cache_dir=os.path.join(cache_dir, 'train') if cache_dir else None,
        force_reload=force_reload
    )
    
    val_dataset = BatchSmartInsoleDataset(
        val_files,
        seq_length=1,
        cache_dir=os.path.join(cache_dir, 'val') if cache_dir else None,
        force_reload=force_reload
    )
    
    test_dataset = BatchSmartInsoleDataset(
        test_files,
        seq_length=1,
        cache_dir=os.path.join(cache_dir, 'test') if cache_dir else None,
        force_reload=force_reload
    )
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    train_stats = train_dataset.get_statistics()
    val_stats = val_dataset.get_statistics()
    test_stats = test_dataset.get_statistics()
    
    print(f"\nğŸ“Š æ•°æ®é›†ç»Ÿè®¡:")
    print(f"  è®­ç»ƒé›†: {train_stats['n_samples']:,} æ ·æœ¬")
    print(f"  éªŒè¯é›†: {val_stats['n_samples']:,} æ ·æœ¬")
    print(f"  æµ‹è¯•é›†: {test_stats['n_samples']:,} æ ·æœ¬")
    print(f"  ç‰¹å¾ç»´åº¦: {train_stats['feature_shape']} (åº”ä¸º17)")
    print(f"  æ ‡ç­¾ç»´åº¦: {train_stats['label_shape']} (åº”ä¸º3)")
    
    # 5. åˆ›å»ºæ•°æ®åŠ è½½å™¨
    print(f"\nâš¡ åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=0,  # å¯ä»¥è®¾ä¸ºCPUæ ¸å¿ƒæ•°ï¼Œä½†æ³¨æ„å†…å­˜ä½¿ç”¨
        pin_memory=True,
        drop_last=False
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=0,
        pin_memory=True,
        drop_last=False
    )
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=0,
        pin_memory=True,
        drop_last=False
    )
    
    # 6. éªŒè¯ç¬¬ä¸€ä¸ªæ‰¹æ¬¡
    print(f"\nğŸ§ª éªŒè¯æ•°æ®åŠ è½½å™¨...")
    if train_loader:
        batch = next(iter(train_loader))
    
        cap_features = batch['cap_features']
        imu_features = batch['imu_features']
        labels = batch['labels']
        
        print(f"  æ‰¹é‡CapSenseç‰¹å¾å½¢çŠ¶: {cap_features.shape}")
        print(f"  æ‰¹é‡IMUç‰¹å¾å½¢çŠ¶: {imu_features.shape}")
        print(f"  æ‰¹é‡æ ‡ç­¾å½¢çŠ¶: {labels.shape}")
        print(f"  CapSenseèŒƒå›´: [{cap_features.min():.3f}, {cap_features.max():.3f}]")
        print(f"  IMUèŒƒå›´: [{imu_features.min():.3f}, {imu_features.max():.3f}]")
        print(f"  æ ‡ç­¾èŒƒå›´: [{labels.min():.3f}, {labels.max():.3f}]")
        
        if cap_features.shape[1] != 12:
            print(f"âš ï¸  è­¦å‘Š: CapSenseç‰¹å¾ç»´åº¦åº”ä¸º12ï¼Œå®é™…ä¸º{cap_features.shape[1]}")
        if imu_features.shape[1] != 5:
            print(f"âš ï¸  è­¦å‘Š: IMUç‰¹å¾ç»´åº¦åº”ä¸º5ï¼Œå®é™…ä¸º{imu_features.shape[1]}")
        
        print(f"\nâœ… æ‰¹é‡æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆ!")
        
        return train_loader, val_loader, test_loader, train_files, val_files, test_files

def save_split_info(train_files, val_files, test_files, output_file='dataset_split_info.json'):
    """ä¿å­˜æ•°æ®é›†åˆ’åˆ†ä¿¡æ¯"""
    split_info = {
        'train_files': [str(f) for f in train_files],
        'val_files': [str(f) for f in val_files],
        'test_files': [str(f) for f in test_files],
        'train_count': len(train_files),
        'val_count': len(val_files),
        'test_count': len(test_files),
        'total_count': len(train_files) + len(val_files) + len(test_files)
    }
    
    with open(output_file, 'w', encoding='utf-8') as f:
        import json
        json.dump(split_info, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ’¾ åˆ’åˆ†ä¿¡æ¯å·²ä¿å­˜åˆ°: {output_file}")
    return output_file