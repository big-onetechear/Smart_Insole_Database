# train_model.py
"""
æ™ºèƒ½é‹å«æ¨¡å‹è®­ç»ƒè„šæœ¬
åŸºäºè®ºæ–‡ã€ŠEstimation of Three-Dimensional Ground Reaction Forces Using Low-Cost Smart Insolesã€‹
"""
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, random_split
import numpy as np
import pandas as pd
import os
import sys
import time
from datetime import datetime
import matplotlib.pyplot as plt
# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from DataLoading import SmartInsoleDataset
from modelAssemb import PaperFusionModel
from LearnMethod import (
    TrainingConfig, 
    SmartTrainingManager,
    ModelEvaluator,
    create_default_config
)

def setup_data_paths(base_dir="."):
    """è®¾ç½®æ•°æ®è·¯å¾„"""
    print("ğŸ“ è®¾ç½®æ•°æ®è·¯å¾„...")
    
    subjects = [f"subject_{i}" for i in range(1, 6)]
    movements = [
        "squatting",
        "walking",  # æˆ– stepping_in_place
        "jogging",  # æˆ– running_in_place  
        "swaying",
        "jump_inplace",
        "jump_fb"   # forward_backward
    ]
    
    # æŸ¥æ‰¾æ‰€æœ‰CSVæ–‡ä»¶
    data_files = []
    for subject in subjects:
        subject_dir = os.path.join(base_dir, subject)
        if os.path.exists(subject_dir):
            for file in os.listdir(subject_dir):
                if file.endswith('_merged.csv'):
                    movement = file.split('_')[0]
                    data_files.append({
                        'subject': subject,
                        'movement': movement,
                        'path': os.path.join(subject_dir, file),
                        'size': os.path.getsize(os.path.join(subject_dir, file))
                    })
    
    print(f"âœ… æ‰¾åˆ° {len(data_files)} ä¸ªæ•°æ®æ–‡ä»¶")
    for i, file_info in enumerate(data_files[:5]):  # æ˜¾ç¤ºå‰5ä¸ª
        print(f"  {i+1}. {file_info['subject']}/{file_info['movement']}: {file_info['size']/1024/1024:.1f} MB")
    
    return data_files

def load_and_split_dataset(csv_path, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):
    """åŠ è½½æ•°æ®é›†å¹¶åˆ’åˆ†"""
    print(f"\nğŸ“Š åŠ è½½æ•°æ®é›†: {os.path.basename(csv_path)}")
    
    # åˆ›å»ºæ•°æ®é›†
    dataset = SmartInsoleDataset(csv_path)
    
    print(f"   æ•°æ®é›†å¤§å°: {len(dataset)}")
    print(f"   CapSenseç‰¹å¾: {dataset[0][0].shape}")
    print(f"   IMUç‰¹å¾: {dataset[0][1].shape}")
    print(f"   æ ‡ç­¾: {dataset[0][2].shape}")
    
    # è®¡ç®—åˆ’åˆ†å¤§å°
    n_total = len(dataset)
    n_train = int(n_total * train_ratio)
    n_val = int(n_total * val_ratio)
    n_test = n_total - n_train - n_val
    
    # éšæœºåˆ’åˆ†
    train_dataset, val_dataset, test_dataset = random_split(
        dataset, [n_train, n_val, n_test],
        generator=torch.Generator().manual_seed(42)  # å›ºå®šéšæœºç§å­
    )
    
    print(f"   è®­ç»ƒé›†: {len(train_dataset)}")
    print(f"   éªŒè¯é›†: {len(val_dataset)}")
    print(f"   æµ‹è¯•é›†: {len(test_dataset)}")
    
    return train_dataset, val_dataset, test_dataset

def create_data_loaders(train_dataset, val_dataset, test_dataset, batch_size=64):
    """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
    train_loader = DataLoader(
        train_dataset, 
        batch_size=batch_size, 
        shuffle=True,
        num_workers=0  # Windowsä¸Šè®¾ä¸º0é¿å…é—®é¢˜
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=0
    )
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=0
    )
    
    return train_loader, val_loader, test_loader

def train_single_subject(subject_data, config, save_dir="results"):
    """è®­ç»ƒå•ä¸ªå—è¯•è€…çš„æ¨¡å‹"""
    print(f"\nğŸ¯ å¼€å§‹è®­ç»ƒ {subject_data['subject']}...")
    
    # åŠ è½½æ•°æ®
    train_dataset, val_dataset, test_dataset = load_and_split_dataset(
        subject_data['path']
    )
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader, val_loader, test_loader = create_data_loaders(
        train_dataset, val_dataset, test_dataset, 
        batch_size=config.batch_size
    )
    
    # åˆ›å»ºæ¨¡å‹
    model = PaperFusionModel()
    
    # åˆ›å»ºè®­ç»ƒç®¡ç†å™¨
    manager = SmartTrainingManager(model, config)
    
    # è®­ç»ƒ
    start_time = time.time()
    history = manager.fit(train_loader, val_loader)
    training_time = time.time() - start_time
    
    # è¯„ä¼°
    print(f"\nğŸ“ˆ è¯„ä¼°æ¨¡å‹...")
    eval_results = ModelEvaluator.evaluate(
        model, test_loader, config.device
    )
    
    # ä¿å­˜ç»“æœ
    subject_name = subject_data['subject']
    movement_name = subject_data['movement']
    
    results = {
        'subject': subject_name,
        'movement': movement_name,
        'model': model,
        'history': history,
        'eval_results': eval_results,
        'config': config.to_dict(),
        'training_time': training_time,
        'model_params': sum(p.numel() for p in model.parameters())
    }
    
    # ä¿å­˜æ¨¡å‹å’Œç»“æœ
    os.makedirs(save_dir, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    model_path = os.path.join(save_dir, f"{subject_name}_{movement_name}_{timestamp}.pth")
    results_path = os.path.join(save_dir, f"results_{subject_name}_{movement_name}_{timestamp}.pkl")
    
    # ä¿å­˜æ£€æŸ¥ç‚¹
    manager.save_checkpoint(model_path)
    
    # ä¿å­˜ç»“æœ
    torch.save(results, results_path)
    
    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜:")
    print(f"   æ¨¡å‹: {model_path}")
    print(f"   ç»“æœ: {results_path}")
    
    return results

def plot_training_history(history, save_path=None):
    """ç»˜åˆ¶è®­ç»ƒå†å²"""
    fig, axes = plt.subplots(2, 2, figsize=(12, 8))
    
    # è®­ç»ƒ/éªŒè¯æŸå¤±
    axes[0, 0].plot(history['train_loss'], label='è®­ç»ƒæŸå¤±')
    axes[0, 0].plot(history['val_loss'], label='éªŒè¯æŸå¤±')
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('Loss')
    axes[0, 0].set_title('è®­ç»ƒå’ŒéªŒè¯æŸå¤±')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)
    
    # å­¦ä¹ ç‡
    axes[0, 1].plot(history['learning_rate'], color='green')
    axes[0, 1].set_xlabel('Epoch')
    axes[0, 1].set_ylabel('Learning Rate')
    axes[0, 1].set_title('å­¦ä¹ ç‡å˜åŒ–')
    axes[0, 1].grid(True, alpha=0.3)
    
    # è®­ç»ƒæ—¶é—´
    axes[1, 0].plot(history['train_time'], label='è®­ç»ƒæ—¶é—´')
    axes[1, 0].plot(history['val_time'], label='éªŒè¯æ—¶é—´')
    axes[1, 0].set_xlabel('Epoch')
    axes[1, 0].set_ylabel('æ—¶é—´ (ç§’)')
    axes[1, 0].set_title('æ¯è½®è®­ç»ƒæ—¶é—´')
    axes[1, 0].legend()
    axes[1, 0].grid(True, alpha=0.3)
    
    # æ€»æ—¶é—´ç´¯ç§¯
    cumulative_time = np.cumsum(history['epoch_time'])
    axes[1, 1].plot(cumulative_time)
    axes[1, 1].set_xlabel('Epoch')
    axes[1, 1].set_ylabel('ç´¯è®¡æ—¶é—´ (ç§’)')
    axes[1, 1].set_title('ç´¯è®¡è®­ç»ƒæ—¶é—´')
    axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"ğŸ“Š è®­ç»ƒå›¾å·²ä¿å­˜: {save_path}")
    
    plt.show()

def main():
    """ä¸»è®­ç»ƒæµç¨‹"""
    print("=" * 70)
    print("ğŸš€ æ™ºèƒ½é‹å« - 3Dåœ°é¢åä½œç”¨åŠ›ä¼°è®¡æ¨¡å‹è®­ç»ƒ")
    print("=" * 70)
    
    # 1. è®¾ç½®
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_dir = f"training_results_{timestamp}"
    os.makedirs(results_dir, exist_ok=True)
    
    print(f"ğŸ“ ç»“æœç›®å½•: {results_dir}")
    
    # 2. æŸ¥æ‰¾æ•°æ®æ–‡ä»¶
    data_files = setup_data_paths()
    
    if not data_files:
        print("âŒ æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥è·¯å¾„")
        return
    
    # 3. é€‰æ‹©è¦è®­ç»ƒçš„æ•°æ®
    print("\nğŸ“‹ å¯ç”¨æ•°æ®æ–‡ä»¶:")
    for i, file_info in enumerate(data_files):
        print(f"  [{i+1}] {file_info['subject']} - {file_info['movement']}")
    
    # å…ˆæµ‹è¯•ä¸€ä¸ªæ–‡ä»¶
    print("\nğŸ”§ å…ˆæµ‹è¯•ç¬¬ä¸€ä¸ªæ–‡ä»¶...")
    test_file = data_files[0]  # subject_1 squatting
    
    # 4. åˆ›å»ºé…ç½®ï¼ˆä½¿ç”¨è®ºæ–‡å‚æ•°ï¼‰
    config = TrainingConfig(
        # è®ºæ–‡å‚æ•°
        batch_size=64,
        learning_rate=0.0001,  # è®ºæ–‡ä½¿ç”¨0.0001
        weight_decay=1e-8,     # è®ºæ–‡ä½¿ç”¨1e-8
        epochs=50,             # å¯ä»¥å…ˆå°‘ä¸€äº›å¿«é€Ÿæµ‹è¯•
        lr_scheduler_type='plateau',
        lr_patience=3,
        lr_factor=0.1,
        
        # æ—©åœ
        use_early_stopping=True,
        early_stop_patience=10,
        
        # å…¶ä»–
        loss_function='mse',
        optimizer='adam',
        verbose=True,
        device='cuda' if torch.cuda.is_available() else 'cpu'
    )
    
    print("\nâš™ï¸ è®­ç»ƒé…ç½®:")
    print(config)
    
    # 5. è®­ç»ƒ
    print(f"\nğŸ¬ å¼€å§‹è®­ç»ƒ: {test_file['subject']} - {test_file['movement']}")
    results = train_single_subject(test_file, config, results_dir)
    
    # 6. æ˜¾ç¤ºç»“æœ
    print("\n" + "=" * 70)
    print("ğŸ“Š è®­ç»ƒç»“æœæ‘˜è¦")
    print("=" * 70)
    
    print(f"å—è¯•è€…: {results['subject']}")
    print(f"è¿åŠ¨ç±»å‹: {results['movement']}")
    print(f"è®­ç»ƒæ—¶é—´: {results['training_time']:.2f} ç§’")
    print(f"æ¨¡å‹å‚æ•°é‡: {results['model_params']:,}")
    print(f"æœ€ä½³éªŒè¯æŸå¤±: {min(results['history']['val_loss']):.6f}")
    
    print("\nğŸ“ˆ æµ‹è¯•é›†è¯„ä¼°:")
    ModelEvaluator.print_metrics(results['eval_results']['metrics'])
    
    # 7. ç»˜åˆ¶å›¾è¡¨
    plot_path = os.path.join(results_dir, f"training_plot_{test_file['subject']}.png")
    plot_training_history(results['history'], plot_path)
    
    # 8. ä¿å­˜è®­ç»ƒæŠ¥å‘Š
    report_path = os.path.join(results_dir, "training_report.txt")
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("=" * 70 + "\n")
        f.write("æ™ºèƒ½é‹å«æ¨¡å‹è®­ç»ƒæŠ¥å‘Š\n")
        f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write("=" * 70 + "\n\n")
        
        f.write(f"å—è¯•è€…: {results['subject']}\n")
        f.write(f"è¿åŠ¨ç±»å‹: {results['movement']}\n")
        f.write(f"æ•°æ®æ–‡ä»¶: {test_file['path']}\n")
        f.write(f"è®­ç»ƒæ—¶é—´: {results['training_time']:.2f} ç§’\n")
        f.write(f"æ€»è½®æ•°: {len(results['history']['train_loss'])}\n\n")
        
        f.write("æ¨¡å‹é…ç½®:\n")
        for key, value in results['config'].items():
            f.write(f"  {key}: {value}\n")
        
        f.write("\nè¯„ä¼°ç»“æœ:\n")
        metrics = results['eval_results']['metrics']
        for key, value in metrics.items():
            if isinstance(value, list):
                f.write(f"  {key}: {value}\n")
            else:
                f.write(f"  {key}: {value:.6f}\n")
    
    print(f"\nğŸ“„ è®­ç»ƒæŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    
    print("\n" + "=" * 70)
    print("âœ… è®­ç»ƒå®Œæˆï¼")
    print("=" * 70)

if __name__ == "__main__":
    main()