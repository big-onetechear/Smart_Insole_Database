# check_all_normalized.py

import glob
import json
from pathlib import Path
# check_normalized_data.py
import pandas as pd
import numpy as np
import os
from pathlib import Path

def validate_normalized_data(data_path):
    """éªŒè¯å½’ä¸€åŒ–æ•°æ®æ˜¯å¦ç¬¦åˆæ¨¡å‹è¦æ±‚"""
    
    print("="*60)
    print("ğŸ” å½’ä¸€åŒ–æ•°æ®éªŒè¯")
    print("="*60)
    
    # è¯»å–æ•°æ®
    df = pd.read_csv(data_path)
    
    print(f"ğŸ“Š æ•°æ®åŸºæœ¬ä¿¡æ¯:")
    print(f"  æ–‡ä»¶: {Path(data_path).name}")
    print(f"  æ ·æœ¬æ•°: {len(df):,}")
    print(f"  ç‰¹å¾æ•°: {len(df.columns)}")
    
    # 1. æ£€æŸ¥æ•°æ®èŒƒå›´ [0, 1]
    print(f"\nğŸ“ˆ æ•°æ®èŒƒå›´æ£€æŸ¥:")
    violations = []
    for col in df.columns:
        col_min = df[col].min()
        col_max = df[col].max()
        
        # å…è®¸å¾®å°è¯¯å·®
        if col_min < -0.05 or col_max > 1.05:
            violations.append((col, col_min, col_max))
        else:
            print(f"  âœ“ {col}: [{col_min:.3f}, {col_max:.3f}]")
    
    if violations:
        print(f"  âš ï¸  ä»¥ä¸‹åˆ—è¶…å‡º[0,1]èŒƒå›´:")
        for col, cmin, cmax in violations:
            print(f"    {col}: [{cmin:.3f}, {cmax:.3f}]")
    
    # 2. æ£€æŸ¥ç¼ºå¤±å€¼
    print(f"\nâ“ ç¼ºå¤±å€¼æ£€æŸ¥:")
    missing_count = df.isnull().sum().sum()
    if missing_count > 0:
        print(f"  âš ï¸  å‘ç° {missing_count} ä¸ªç¼ºå¤±å€¼")
        missing_cols = df.columns[df.isnull().any()].tolist()
        print(f"    æœ‰ç¼ºå¤±çš„åˆ—: {missing_cols}")
    else:
        print(f"  âœ“ æ— ç¼ºå¤±å€¼")
    
    # 3. æ£€æŸ¥ç‰¹å¾å’Œæ ‡ç­¾
    print(f"\nğŸ·ï¸  ç‰¹å¾/æ ‡ç­¾è¯†åˆ«:")
    
    # æ ¹æ®ä½ çš„åˆ—åæ¨¡å¼è‡ªåŠ¨è¯†åˆ«
    feature_cols = []
    label_cols = []
    
    for col in df.columns:
        if col.startswith('C') and col[1:].isdigit():  # C0-C11
            feature_cols.append(col)
        elif col in ['Ax', 'Ay', 'Az', 'Gr', 'Gp']:
            feature_cols.append(col)
        elif '_norm' in col:
            label_cols.append(col)
    
    print(f"  ç‰¹å¾åˆ— ({len(feature_cols)}ä¸ª): {feature_cols}")
    print(f"  æ ‡ç­¾åˆ— ({len(label_cols)}ä¸ª): {label_cols}")
    
    # 4. æ£€æŸ¥æ•°æ®åˆ†å¸ƒ
    print(f"\nğŸ“Š æ•°æ®åˆ†å¸ƒç»Ÿè®¡:")
    print("  ç‰¹å¾ç»Ÿè®¡:")
    for col in feature_cols[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªç‰¹å¾
        mean_val = df[col].mean()
        std_val = df[col].std()
        print(f"    {col}: å‡å€¼={mean_val:.3f}, æ ‡å‡†å·®={std_val:.3f}")
    
    # 5. ä¿å­˜éªŒè¯æŠ¥å‘Š
    report = {
        "file": str(data_path),
        "samples": len(df),
        "features": len(feature_cols),
        "labels": len(label_cols),
        "range_violations": len(violations),
        "missing_values": missing_count,
        "feature_columns": feature_cols,
        "label_columns": label_cols,
        "feature_stats": {
            col: {
                "mean": float(df[col].mean()),
                "std": float(df[col].std()),
                "min": float(df[col].min()),
                "max": float(df[col].max())
            }
            for col in feature_cols[:10]  # åªä¿å­˜å‰10ä¸ªç‰¹å¾çš„è¯¦ç»†ç»Ÿè®¡
        }
    }
    
    return report

def check_all_normalized_files(base_path="Smart_Insole_Database"):
    """æ£€æŸ¥æ‰€æœ‰å½’ä¸€åŒ–æ–‡ä»¶"""
    
    all_reports = {}
    issues = []
    
    # æŸ¥æ‰¾æ‰€æœ‰å½’ä¸€åŒ–æ–‡ä»¶
    pattern = f"{base_path}/subjectRepro*/norm/*_normalized.csv"
    normalized_files = glob.glob(pattern)
    
   
    print(f"æ‰¾åˆ° {len(normalized_files)} ä¸ªå½’ä¸€åŒ–æ–‡ä»¶")
    print("\nğŸ“‚ æ‰¾åˆ°çš„æ–‡ä»¶åˆ—è¡¨:")
    for i, file_path in enumerate(sorted(normalized_files), 1):
        file_name = Path(file_path).name
        folder = Path(file_path).parent.name  # normæ–‡ä»¶å¤¹
        subject_folder = Path(file_path).parent.parent.name  # subjectReproXæ–‡ä»¶å¤¹
        
        print(f"  {i:2d}. {subject_folder}/{folder}/{file_name}")
    for file_path in normalized_files:
        print(f"\n{'='*60}")
        print(f"æ£€æŸ¥: {Path(file_path).name}")
        
        try:
            report = validate_normalized_data(file_path)
            all_reports[file_path] = report
            
            # è®°å½•é—®é¢˜
            if report['range_violations'] > 0 or report['missing_values'] > 0:
                issues.append({
                    'file': file_path,
                    'range_violations': report['range_violations'],
                    'missing_values': report['missing_values']
                })
                
        except Exception as e:
            print(f"âŒ æ£€æŸ¥å¤±è´¥: {e}")
            issues.append({
                'file': file_path,
                'error': str(e)
            })
    
    # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
    print(f"\n{'='*60}")
    print("ğŸ“Š æ€»ä½“æ£€æŸ¥æŠ¥å‘Š")
    print(f"{'='*60}")
    
    total_files = len(normalized_files)
    files_with_issues = len(issues)
    
    print(f"æ€»æ–‡ä»¶æ•°: {total_files}")
    print(f"æœ‰é—®é¢˜çš„æ–‡ä»¶: {files_with_issues}")
    print(f"é€šè¿‡ç‡: {(total_files - files_with_issues)/total_files*100:.1f}%")
    
    if issues:
        print(f"\nâŒ é—®é¢˜æ–‡ä»¶:")
        for issue in issues:
            print(f"  â€¢ {Path(issue['file']).name}")
            if 'error' in issue:
                print(f"    é”™è¯¯: {issue['error']}")
            else:
                if issue['range_violations'] > 0:
                    print(f"    èŒƒå›´è¿è§„: {issue['range_violations']}åˆ—")
                if issue['missing_values'] > 0:
                    print(f"    ç¼ºå¤±å€¼: {issue['missing_values']}ä¸ª")
    
    # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
    report_file = "normalized_data_validation_report.json"
    with open(report_file, 'w') as f:
        json.dump(all_reports, f, indent=2)
    
    print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
    
    return all_reports, issues

# åœ¨æ–‡ä»¶æœ«å°¾æ·»åŠ 
if __name__ == "__main__":
    # æ£€æŸ¥æ‰€æœ‰æ–‡ä»¶
    reports, issues = check_all_normalized_files()
    print("æ£€æŸ¥å®Œæˆï¼")