# train_model.py
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm
import os
import time
from datetime import datetime

# å¯¼å…¥æ¨¡å‹å’Œæ•°æ®
from model_architecture import DualStreamAttentionModel
from SmartInsoleDataset_batch import create_batch_data_loaders

class Trainer:
    def __init__(self, model, train_loader, val_loader, test_loader, config):
        self.model = model
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.test_loader = test_loader
        self.config = config
        
        # è®¾å¤‡
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model.to(self.device)
        
        # ä¼˜åŒ–å™¨
        self.optimizer = optim.Adam(
            model.parameters(), 
            lr=config['learning_rate'],
            weight_decay=config['weight_decay']
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, 
            mode='min', 
            factor=0.1, 
            patience=3,
            verbose=True
        )
        
        # æŸå¤±å‡½æ•°
        self.criterion = nn.MSELoss()
        
        # è®°å½•
        self.train_losses = []
        self.val_losses = []
        self.best_val_loss = float('inf')
        self.best_model_state = None
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        os.makedirs(config['save_dir'], exist_ok=True)
        
        print(f"ğŸš€ è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"  è®¾å¤‡: {self.device}")
        print(f"  è®­ç»ƒæ ·æœ¬: {len(train_loader.dataset):,}")
        print(f"  éªŒè¯æ ·æœ¬: {len(val_loader.dataset):,}")
        print(f"  æµ‹è¯•æ ·æœ¬: {len(test_loader.dataset):,}")
    
    def train_epoch(self, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0
        num_batches = 0
        
        pbar = tqdm(self.train_loader, desc=f"è®­ç»ƒ Epoch {epoch+1}")
        for batch_idx, batch in enumerate(pbar):
            # ç§»åŠ¨åˆ°è®¾å¤‡
            cap_features = batch['cap_features'].to(self.device)
            imu_features = batch['imu_features'].to(self.device)
            labels = batch['labels'].to(self.device)
            
            # å‰å‘ä¼ æ’­
            self.optimizer.zero_grad()
            outputs = self.model(cap_features, imu_features)
            loss = self.criterion(outputs, labels)
            
            # åå‘ä¼ æ’­
            loss.backward()
            self.optimizer.step()
            
            # è®°å½•
            total_loss += loss.item()
            num_batches += 1
            
            # æ›´æ–°è¿›åº¦æ¡
            avg_loss = total_loss / num_batches
            pbar.set_postfix({'loss': f'{avg_loss:.6f}'})
            
            # æ¯100ä¸ªæ‰¹æ¬¡è®°å½•ä¸€æ¬¡
            if batch_idx % 100 == 0:
                current_lr = self.optimizer.param_groups[0]['lr']
                print(f"   æ‰¹æ¬¡ {batch_idx:4d}/{len(self.train_loader)} | "
                      f"Loss: {loss.item():.6f} | LR: {current_lr:.6f}")
        
        avg_train_loss = total_loss / num_batches
        self.train_losses.append(avg_train_loss)
        
        return avg_train_loss
    
    def validate(self):
        """éªŒè¯"""
        self.model.eval()
        total_loss = 0
        num_batches = 0
        
        with torch.no_grad():
            pbar = tqdm(self.val_loader, desc="éªŒè¯")
            for batch in pbar:
                cap_features = batch['cap_features'].to(self.device)
                imu_features = batch['imu_features'].to(self.device)
                labels = batch['labels'].to(self.device)
                
                outputs = self.model(cap_features, imu_features)
                loss = self.criterion(outputs, labels)
                
                total_loss += loss.item()
                num_batches += 1
                
                avg_loss = total_loss / num_batches
                pbar.set_postfix({'val_loss': f'{avg_loss:.6f}'})
        
        avg_val_loss = total_loss / num_batches
        self.val_losses.append(avg_val_loss)
        
        return avg_val_loss
    
    def test(self):
        """æµ‹è¯•"""
        self.model.eval()
        total_loss = 0
        num_batches = 0
        
        # å­˜å‚¨é¢„æµ‹å’ŒçœŸå®å€¼ç”¨äºåˆ†æ
        all_predictions = []
        all_labels = []
        
        with torch.no_grad():
            pbar = tqdm(self.test_loader, desc="æµ‹è¯•")
            for batch in pbar:
                cap_features = batch['cap_features'].to(self.device)
                imu_features = batch['imu_features'].to(self.device)
                labels = batch['labels'].to(self.device)
                
                outputs = self.model(cap_features, imu_features)
                loss = self.criterion(outputs, labels)
                
                total_loss += loss.item()
                num_batches += 1
                
                # æ”¶é›†æ•°æ®ç”¨äºåˆ†æ
                all_predictions.append(outputs.cpu())
                all_labels.append(labels.cpu())
                
                avg_loss = total_loss / num_batches
                pbar.set_postfix({'test_loss': f'{avg_loss:.6f}'})
        
        avg_test_loss = total_loss / num_batches
        
        # è®¡ç®—NRMSEï¼ˆè®ºæ–‡ä¸­çš„è¯„ä»·æŒ‡æ ‡ï¼‰
        all_predictions = torch.cat(all_predictions, dim=0)
        all_labels = torch.cat(all_labels, dim=0)
        
        # è®¡ç®—æ¯ä¸ªGRFåˆ†é‡çš„NRMSE
        mse = nn.MSELoss(reduction='none')(all_predictions, all_labels).mean(dim=0)
        rmse = torch.sqrt(mse)
        
        # å½’ä¸€åŒ–ï¼ˆé™¤ä»¥æ ‡ç­¾èŒƒå›´ï¼‰
        label_range = all_labels.max(dim=0)[0] - all_labels.min(dim=0)[0]
        nrmse = (rmse / label_range) * 100  # ç™¾åˆ†æ¯”
        
        return avg_test_loss, nrmse, all_predictions, all_labels
    
    def save_checkpoint(self, epoch, val_loss, is_best=False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'best_val_loss': self.best_val_loss,
            'config': self.config
        }
        
        # ä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹
        checkpoint_path = os.path.join(
            self.config['save_dir'], 
            f'checkpoint_epoch_{epoch+1}.pth'
        )
        torch.save(checkpoint, checkpoint_path)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if is_best:
            best_path = os.path.join(self.config['save_dir'], 'best_model.pth')
            torch.save(checkpoint, best_path)
            print(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹: {best_path}")
    
    def plot_losses(self):
        """ç»˜åˆ¶æŸå¤±æ›²çº¿"""
        plt.figure(figsize=(10, 6))
        plt.plot(self.train_losses, label='è®­ç»ƒæŸå¤±', marker='o', markersize=3)
        plt.plot(self.val_losses, label='éªŒè¯æŸå¤±', marker='s', markersize=3)
        plt.xlabel('Epoch')
        plt.ylabel('MSE Loss')
        plt.title('è®­ç»ƒå’ŒéªŒè¯æŸå¤±æ›²çº¿')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # ä¿å­˜å›¾åƒ
        plot_path = os.path.join(self.config['save_dir'], 'loss_curve.png')
        plt.savefig(plot_path, dpi=150, bbox_inches='tight')
        plt.show()
        print(f"ğŸ“Š æŸå¤±æ›²çº¿å·²ä¿å­˜: {plot_path}")
    
    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        print(f"\n{'='*80}")
        print("ğŸš€ å¼€å§‹è®­ç»ƒ!")
        print(f"{'='*80}")
        
        start_time = time.time()
        
        for epoch in range(self.config['num_epochs']):
            print(f"\nğŸ“ˆ Epoch {epoch+1}/{self.config['num_epochs']}")
            
            # è®­ç»ƒ
            train_loss = self.train_epoch(epoch)
            print(f"  è®­ç»ƒæŸå¤±: {train_loss:.6f}")
            
            # éªŒè¯
            val_loss = self.validate()
            print(f"  éªŒè¯æŸå¤±: {val_loss:.6f}")
            
            # å­¦ä¹ ç‡è°ƒåº¦
            self.scheduler.step(val_loss)
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_loss < self.best_val_loss:
                self.best_val_loss = val_loss
                self.best_model_state = self.model.state_dict().copy()
                self.save_checkpoint(epoch, val_loss, is_best=True)
                print(f"ğŸ¯ æ–°çš„æœ€ä½³éªŒè¯æŸå¤±: {val_loss:.6f}")
            
            # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
            if (epoch + 1) % self.config['save_interval'] == 0:
                self.save_checkpoint(epoch, val_loss)
            
            # æå‰ç»ˆæ­¢æ£€æŸ¥
            if epoch >= 10 and val_loss > np.mean(self.val_losses[-5:]):
                print("âš ï¸  éªŒè¯æŸå¤±ä¸Šå‡ï¼Œè€ƒè™‘æå‰ç»ˆæ­¢...")
        
        # è®­ç»ƒå®Œæˆ
        training_time = time.time() - start_time
        print(f"\nâœ… è®­ç»ƒå®Œæˆ! ç”¨æ—¶: {training_time:.2f}ç§’")
        
        # åŠ è½½æœ€ä½³æ¨¡å‹
        if self.best_model_state is not None:
            self.model.load_state_dict(self.best_model_state)
            print("ğŸ” å·²åŠ è½½æœ€ä½³æ¨¡å‹")
        
        # ç»˜åˆ¶æŸå¤±æ›²çº¿
        self.plot_losses()
        
        # æœ€ç»ˆæµ‹è¯•
        print(f"\n{'='*80}")
        print("ğŸ§ª æœ€ç»ˆæµ‹è¯•")
        print(f"{'='*80}")
        
        test_loss, nrmse, predictions, labels = self.test()
        
        print(f"ğŸ“Š æµ‹è¯•ç»“æœ:")
        print(f"  MSE Loss: {test_loss:.6f}")
        print(f"  NRMSE (Fx): {nrmse[0].item():.2f}%")
        print(f"  NRMSE (Fy): {nrmse[1].item():.2f}%")
        print(f"  NRMSE (Fz): {nrmse[2].item():.2f}%")
        print(f"  NRMSE å¹³å‡: {nrmse.mean().item():.2f}%")
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_path = os.path.join(self.config['save_dir'], 'final_model.pth')
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'config': self.config,
            'test_loss': test_loss,
            'nrmse': nrmse,
            'predictions': predictions,
            'labels': labels
        }, final_path)
        print(f"ğŸ’¾ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {final_path}")
        
        return test_loss, nrmse

def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®
    config = {
        'learning_rate': 0.0001,      # è®ºæ–‡ä¸­ä½¿ç”¨çš„å­¦ä¹ ç‡
        'weight_decay': 1e-8,         # è®ºæ–‡ä¸­çš„æƒé‡è¡°å‡
        'num_epochs': 50,             # è®­ç»ƒè½®æ•°
        'batch_size': 32,             # æ‰¹é‡å¤§å°
        'save_dir': './checkpoints',  # ä¿å­˜ç›®å½•
        'save_interval': 5,           # ä¿å­˜é—´éš”
    }
    
    print("="*80)
    print("ğŸ¤– GRFé¢„æµ‹æ¨¡å‹è®­ç»ƒ")
    print("="*80)
    
    # 1. åŠ è½½æ•°æ®
    print("\nğŸ“¥ åŠ è½½æ•°æ®...")
    train_loader, val_loader, test_loader, _, _, _ = create_batch_data_loaders(
        base_path='D:/TG0/PublicData_Rep/Smart_Insole_Database',
        split_method='mixed',
        batch_size=config['batch_size'],
        cache_dir='./data_cache',
        force_reload=False  # ä½¿ç”¨ç¼“å­˜åŠ é€Ÿ
    )
    
    # 2. åˆ›å»ºæ¨¡å‹
    print("\nğŸ§  åˆ›å»ºæ¨¡å‹...")
    model = DualStreamAttentionModel(
        cap_dim=12,
        imu_dim=5,
        hidden_dim=32,
        output_dim=3
    )
    
    print(f"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    
    # 3. åˆ›å»ºè®­ç»ƒå™¨
    trainer = Trainer(model, train_loader, val_loader, test_loader, config)
    
    # 4. è®­ç»ƒ
    test_loss, nrmse = trainer.train()
    
    # 5. ä¸è®ºæ–‡ç»“æœå¯¹æ¯”
    print(f"\n{'='*80}")
    print("ğŸ“Š ä¸è®ºæ–‡ç»“æœå¯¹æ¯”")
    print(f"{'='*80}")
    print(f"æˆ‘ä»¬çš„ç»“æœ:")
    print(f"  NRMSE å¹³å‡: {nrmse.mean().item():.2f}%")
    print(f"  NRMSE (Fx, Fy, Fz): {nrmse[0].item():.2f}%, {nrmse[1].item():.2f}%, {nrmse[2].item():.2f}%")
    print(f"\nè®ºæ–‡ç»“æœ (Table 1):")
    print(f"  Best NRMSE: 4.16%")
    print(f"  å…¶ä»–æ–¹æ³•: 8.46%-20%")
    
    if nrmse.mean().item() <= 5.0:
        print(f"\nğŸ‰ æˆåŠŸå¤ç°è®ºæ–‡ç»“æœ!")
    else:
        print(f"\nâš ï¸  ä¸è®ºæ–‡ç»“æœæœ‰å·®è·ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´è¶…å‚æ•°æˆ–æ¨¡å‹ç»“æ„")

if __name__ == "__main__":
    main()